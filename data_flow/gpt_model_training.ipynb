{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26c247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_CSV = ../datasets/train_raw.csv\n",
      "bf16 supported: True\n"
     ]
    }
   ],
   "source": [
    "import os, platform, pathlib, torch, random, numpy as np\n",
    "\n",
    "# --- Windows-safe env flags ---\n",
    "if platform.system() == \"Windows\":\n",
    "    os.environ[\"HF_DATASETS_DISABLE_MULTIPROCESSING\"] = \"1\"  # без сабпроцессов в datasets\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"           # без параллелизма токенайзера\n",
    "    os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"              # <--- ОТКЛЮЧИТЬ torch.compile в Unsloth\n",
    "    os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = r\"C:\\ti_cache\"   # короткий путь для inductor\n",
    "    os.environ[\"TRITON_CACHE_DIR\"]       = r\"C:\\triton_cache\"\n",
    "    pathlib.Path(r\"C:\\ti_cache\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(r\"C:\\triton_cache\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths (we'll pick the first that exists)\n",
    "CANDIDATE_CSVS = [\n",
    "    \"/mnt/data/train_raw.csv\",\n",
    "    \"../datasets/train_raw.csv\",\n",
    "    \"train_raw.csv\",\n",
    "]\n",
    "DATA_CSV = next((p for p in CANDIDATE_CSVS if pathlib.Path(p).exists()), CANDIDATE_CSVS[-1])\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"gptoss20b_ner_bio_unsloth\"\n",
    "TRAIN_JSONL = \"train.jsonl\"\n",
    "VAL_JSONL   = \"val.jsonl\"\n",
    "\n",
    "# Training knobs\n",
    "VAL_FRAC = 0.10\n",
    "MAX_LEN  = 1024     # если будет OOM, снизь до 768/512\n",
    "RANK     = 8\n",
    "PACKING  = False    # на Win сначала False\n",
    "GRAD_ACC = 16\n",
    "FORCE_REGEN = False\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"DATA_CSV =\", DATA_CSV)\n",
    "print(\"bf16 supported:\", torch.cuda.is_bf16_supported())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5b673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV: ../datasets/train_raw.csv\n",
      "kept 27248 rows; dropped parse_errors=0, span/len_mismatches=3\n",
      "entity_labels: ['BRAND', 'PERCENT', 'TYPE', 'VOLUME']\n",
      "train=24523, val=2725\n",
      "Wrote: train.jsonl and val.jsonl\n"
     ]
    }
   ],
   "source": [
    "import ast, json, math\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "def parse_ann_cell(cell) -> List[Tuple[int,int,str]]:\n",
    "    \"\"\"annotation: list[(start, end, tag)] — safe parse\"\"\"\n",
    "    if isinstance(cell, (list, tuple)):\n",
    "        ann = cell\n",
    "    else:\n",
    "        ann = ast.literal_eval(str(cell))\n",
    "    out = []\n",
    "    for t in ann:\n",
    "        if not (isinstance(t, (list, tuple)) and len(t) == 3):\n",
    "            raise ValueError(f\"Not a 3-tuple: {t}\")\n",
    "        s, e, tag = t\n",
    "        out.append((int(s), int(e), str(tag)))\n",
    "    return out\n",
    "\n",
    "def word_spans_exact_spaces(text: str) -> List[Tuple[int,int]]:\n",
    "    \"\"\"Split by the exact space ' ' only (keeps indices).\"\"\"\n",
    "    spans = []\n",
    "    i, n = 0, len(text)\n",
    "    while i < n:\n",
    "        while i < n and text[i] == ' ':\n",
    "            i += 1\n",
    "        if i >= n: break\n",
    "        j = i\n",
    "        while j < n and text[j] != ' ':\n",
    "            j += 1\n",
    "        spans.append((i, j))  # [i, j)\n",
    "        i = j\n",
    "    return spans\n",
    "\n",
    "def make_jsonl_if_needed(csv_path: str, train_path: str, val_path: str, val_frac: float, force: bool=False):\n",
    "    if (not force) and pathlib.Path(train_path).exists() and pathlib.Path(val_path).exists():\n",
    "        print(\"JSONL already exist; skipping regen. Set FORCE_REGEN=True to rebuild.\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading CSV:\", csv_path)\n",
    "    df = pd.read_csv(csv_path, sep=None, engine=\"python\").dropna(subset=[\"sample\",\"annotation\"]).reset_index(drop=True)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    rows = []\n",
    "    bad_parse = 0\n",
    "    bad_len_or_span = 0\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        text = str(r[\"sample\"])\n",
    "        try:\n",
    "            ann = parse_ann_cell(r[\"annotation\"])\n",
    "        except Exception:\n",
    "            bad_parse += 1\n",
    "            continue\n",
    "\n",
    "        spans = word_spans_exact_spaces(text)\n",
    "        if len(ann) != len(spans):\n",
    "            bad_len_or_span += 1\n",
    "            continue\n",
    "\n",
    "        ok = all((s == ws and e == we) for (s, e, _), (ws, we) in zip(ann, spans))\n",
    "        if not ok:\n",
    "            bad_len_or_span += 1\n",
    "            continue\n",
    "\n",
    "        tags_str = \" \".join(tag for _, _, tag in ann)\n",
    "        rows.append({\"sample\": text, \"tags\": tags_str, \"ann_tuples\": ann})\n",
    "\n",
    "    df2 = pd.DataFrame(rows)\n",
    "    print(f\"kept {len(df2)} rows; dropped parse_errors={bad_parse}, span/len_mismatches={bad_len_or_span}\")\n",
    "    if len(df2) == 0:\n",
    "        raise RuntimeError(\"No valid rows after validation — check indices/spaces.\")\n",
    "\n",
    "    # Entity label names (without 'O')\n",
    "    entity_labels = sorted({\n",
    "        tag.split(\"-\", 1)[-1]\n",
    "        for ann in df2[\"ann_tuples\"]\n",
    "        for (_, _, tag) in ann\n",
    "        if tag != \"O\"\n",
    "    })\n",
    "    print(\"entity_labels:\", entity_labels)\n",
    "\n",
    "    # Train/val split (safe)\n",
    "    if len(df2) == 1:\n",
    "        train_df, val_df = df2.copy(), df2.iloc[[]].copy()\n",
    "    else:\n",
    "        val_n = max(1, int(round(len(df2) * val_frac)))\n",
    "        val_n = min(val_n, len(df2) - 1)\n",
    "        val_df = df2.sample(n=val_n, random_state=SEED)\n",
    "        train_df = df2.drop(val_df.index).reset_index(drop=True)\n",
    "        val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "    print(f\"train={len(train_df)}, val={len(val_df)}\")\n",
    "\n",
    "    def to_messages(text: str, tags: str):\n",
    "        n = len(tags.split())\n",
    "        dev = {\"role\":\"developer\",\"content\":(\n",
    "            f\"You are an NER tagger. Use BIO with labels: {', '.join(entity_labels)}. \"\n",
    "            f\"Return exactly {n} tags separated by a single space — one per whitespace-separated token. \"\n",
    "            \"Allowed: O, B-<LABEL>, I-<LABEL>. Do not add or remove tokens.\"\n",
    "        )}\n",
    "        usr = {\"role\":\"user\",\"content\": text}\n",
    "        asst = {\"role\":\"assistant\",\"content\": tags}\n",
    "        return {\"messages\":[dev, usr, asst]}\n",
    "\n",
    "    def dump_jsonl(df_in: pd.DataFrame, path: str):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, r in df_in.iterrows():\n",
    "                f.write(json.dumps(to_messages(r[\"sample\"], r[\"tags\"]), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    dump_jsonl(train_df, train_path)\n",
    "    dump_jsonl(val_df,   val_path)\n",
    "\n",
    "    # Save entity labels for later reuse (inference)\n",
    "    with open(\"entity_labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(entity_labels, f, ensure_ascii=False)\n",
    "\n",
    "    print(\"Wrote:\", train_path, \"and\", val_path)\n",
    "\n",
    "make_jsonl_if_needed(DATA_CSV, TRAIN_JSONL, VAL_JSONL, VAL_FRAC, force=FORCE_REGEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5a3867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 24526/24526 [00:02<00:00, 11869.51 examples/s]\n",
      "Map: 100%|██████████| 2725/2725 [00:00<00:00, 11109.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None)}\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-09-27\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "Yo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok_for_render = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\", use_fast=True)\n",
    "\n",
    "def map_row_to_text(ex):\n",
    "    txt = tok_for_render.apply_chat_template(\n",
    "        ex[\"messages\"],\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return {\"text\": txt}\n",
    "\n",
    "features = Features({\"text\": Value(\"string\")})\n",
    "train_raw = load_dataset(\"json\", data_files=TRAIN_JSONL, split=\"train\")\n",
    "val_raw   = load_dataset(\"json\", data_files=VAL_JSONL,   split=\"train\")\n",
    "\n",
    "# Avoid stale cache; and don't use multiprocessing on Windows\n",
    "train_ds = train_raw.map(map_row_to_text, remove_columns=train_raw.column_names,\n",
    "                         features=features, load_from_cache_file=False)\n",
    "val_ds   = val_raw.map(map_row_to_text,   remove_columns=val_raw.column_names,\n",
    "                       features=features, load_from_cache_file=False)\n",
    "\n",
    "print(train_ds.features)\n",
    "print(train_ds[0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efc5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lexan\\AppData\\Local\\Temp\\ipykernel_25896\\2665159020.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "Exception in thread Thread-5 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xad in position 7: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0927 18:13:33.274000 25896 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Model & tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Use Unsloth's linearized gpt-oss 20B for QLoRA that fits ≈14 GB\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    load_in_4bit = True,\n",
    "    max_seq_length = MAX_LEN,\n",
    "    dtype = None,            # auto (bf16 if supported)\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Insert LoRA into all linear layers (attn + experts)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,\n",
    "    lora_alpha = 2 * RANK,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules = \"all-linear\",\n",
    "    bias = \"none\",\n",
    ")\n",
    "\n",
    "print(\"Model & tokenizer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9420131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200017}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 24,526 | Num Epochs = 2 | Total steps = 3,066\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 3,981,312 of 20,918,738,496 (0.02% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='503' max='3066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 503/3066 4:41:17 < 23:59:02, 0.03 it/s, Epoch 0.33/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.620300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 46\u001b[0m\n\u001b[0;32m      6\u001b[0m sft_args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[0;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mOUTPUT_DIR,\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     dataset_num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m     37\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     38\u001b[0m     args\u001b[38;5;241m=\u001b[39msft_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     dataset_text_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(OUTPUT_DIR)\n\u001b[0;32m     48\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(OUTPUT_DIR)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\data_flow\\unsloth_compiled_cache\\UnslothSFTTrainer.py:53\u001b[0m, in \u001b[0;36mprepare_for_training_mode.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfor_training()\n\u001b[1;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Return inference mode\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:323\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\data_flow\\unsloth_compiled_cache\\UnslothSFTTrainer.py:1040\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[1;32m-> 1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m<string>:40\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\data_flow\\unsloth_compiled_cache\\UnslothSFTTrainer.py:1029\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1029\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\unsloth\\models\\_utils.py:1315\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[1;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1309\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   1310\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1313\u001b[0m     )\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_old_compute_loss(model, inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\transformers\\trainer.py:4099\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4097\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4098\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4099\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4101\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\peft\\peft_model.py:1850\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1849\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1850\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1851\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1852\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1853\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1854\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1855\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1856\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1857\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1858\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1859\u001b[0m         )\n\u001b[0;32m   1861\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\transformers\\utils\\generic.py:940\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 940\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    942\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\transformers\\models\\gpt_oss\\modeling_gpt_oss.py:663\u001b[0m, in \u001b[0;36mGptOssForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m output_router_logits \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    659\u001b[0m     output_router_logits \u001b[38;5;28;01mif\u001b[39;00m output_router_logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_router_logits\n\u001b[0;32m    660\u001b[0m )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m outputs: MoeModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    664\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    665\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    666\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    667\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    668\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    669\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    670\u001b[0m     output_router_logits\u001b[38;5;241m=\u001b[39moutput_router_logits,\n\u001b[0;32m    671\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    673\u001b[0m )\n\u001b[0;32m    675\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    676\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\unsloth_zoo\\temporary_patches\\gpt_oss.py:1231\u001b[0m, in \u001b[0;36mpatch_GptOssModel.<locals>.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m   1230\u001b[0m     mask \u001b[38;5;241m=\u001b[39m attention_mask[decoder_layer\u001b[38;5;241m.\u001b[39mattention_type] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attention_mask, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m attention_mask\n\u001b[1;32m-> 1231\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m   1232\u001b[0m         hidden_states,\n\u001b[0;32m   1233\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m   1234\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1235\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1236\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1237\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1238\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1240\u001b[0m     )\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disable_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\utils\\checkpoint.py:488\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m         )\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[0;32m    491\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    492\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\autograd\\function.py:576\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:477\u001b[0m, in \u001b[0;36mUnslothCheckpointFunction.forward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39m_requires_gradient: ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 477\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gpu_buffer: MAIN_STREAM\u001b[38;5;241m.\u001b[39mwait_stream(EXTRA_STREAM)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\transformers\\models\\gpt_oss\\modeling_gpt_oss.py:366\u001b[0m, in \u001b[0;36mGptOssDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    364\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    367\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    368\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    369\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    370\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    371\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    372\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    373\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    375\u001b[0m )\n\u001b[0;32m    376\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\unsloth_zoo\\temporary_patches\\gpt_oss.py:940\u001b[0m, in \u001b[0;36mpatch_GptOssAttention.<locals>.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    933\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: KWARGS_TYPE,\n\u001b[0;32m    939\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[1;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_function(\u001b[38;5;28mself\u001b[39m, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\unsloth_zoo\\temporary_patches\\gpt_oss.py:892\u001b[0m, in \u001b[0;36mpatch_GptOssAttention.<locals>.forward_function\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# flex_attention_with_sink only works for training since KV cache is wrong\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# switch to flex_attention_with_sink which allows all to work\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;66;03m# if is_flex_attention_decoding(self, query_states) and has_static_cache:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;66;03m# attn_weights = None\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m--> 892\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mflex_attention_with_sink\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    898\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;66;03m# Weirdly for inference, flex attention returns gibberish\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;66;03m# Most likely due to left padding\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\unsloth_zoo\\flex_attention\\attention_sink.py:232\u001b[0m, in \u001b[0;36mflex_attention_with_sink\u001b[1;34m(self_attn, query, key, value, attention_mask, scale, sliding_window, compile, has_static_cache)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     block_mask \u001b[38;5;241m=\u001b[39m compiled_create_block_mask(mask_mod, bsz, heads_Q, qlen_Q, qlen_KV, device \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 232\u001b[0m attn_output, logsumexp \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mflex_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muncompiled_flex_attention\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# None needed\u001b[39;49;00m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_lse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# log(sum(exp(xi)))\u001b[39;49;00m\n\u001b[0;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m#### 3 versions to add sink tokens ####\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m#### Version 1: Basic reciprocal denominator removal\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# softmax_sum = torch.exp(logsumexp)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m \n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m### Version 3: Most simple uses sigmoid and scale\u001b[39;00m\n\u001b[0;32m    254\u001b[0m sink_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(logsumexp \u001b[38;5;241m-\u001b[39m self_attn\u001b[38;5;241m.\u001b[39msinks\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\attention\\flex_attention.py:1437\u001b[0m, in \u001b[0;36mflex_attention\u001b[1;34m(query, key, value, score_mod, block_mask, scale, enable_gqa, return_lse, kernel_options)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1436\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1437\u001b[0m out, lse \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_flex_attention_hop_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   1439\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_lse:\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:736\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    733\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\nn\\attention\\flex_attention.py:1424\u001b[0m, in \u001b[0;36mflex_attention.<locals>._flex_attention_hop_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebugging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   1419\u001b[0m     make_eager_backend_with_torch_function_mode,\n\u001b[0;32m   1420\u001b[0m )\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# Dynamo is expecting a callable with \"__code__\" attribute.\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;66;03m# We cannot directly pass hop to it. So we wrap it in a dummy function.\u001b[39;00m\n\u001b[1;32m-> 1424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_flex_attention_hop_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flex_attention_hop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_compilation_env():\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m<eval_with_key>.6:18\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, L_args_0_, L_args_1_, L_args_2_, L_args_4_2_, L_args_4_3_, L_args_4_4_, L_args_4_5_, L_args_4_6_, L_args_4_7_, L_args_4_8_, L_args_4_9_)\u001b[0m\n\u001b[0;32m     16\u001b[0m score_mod_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_mod_0\n\u001b[0;32m     17\u001b[0m mask_fn_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_fn_0\n\u001b[1;32m---> 18\u001b[0m flex_attention \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhigher_order\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflex_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_args_0_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_mod_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_3_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_4_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_5_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_6_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_7_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_8_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_args_4_9_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_fn_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.125\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCK_M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCK_N\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCK_M1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCK_N1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCK_M2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCK_N2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPRESCALE_QK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mROWS_GUARANTEED_SAFE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCKS_ARE_CONTIGUOUS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWRITE_DQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOUTPUT_LOGSUMEXP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m;  l_args_0_ \u001b[38;5;241m=\u001b[39m l_args_1_ \u001b[38;5;241m=\u001b[39m l_args_2_ \u001b[38;5;241m=\u001b[39m score_mod_0 \u001b[38;5;241m=\u001b[39m l_args_4_2_ \u001b[38;5;241m=\u001b[39m l_args_4_3_ \u001b[38;5;241m=\u001b[39m l_args_4_4_ \u001b[38;5;241m=\u001b[39m l_args_4_5_ \u001b[38;5;241m=\u001b[39m l_args_4_6_ \u001b[38;5;241m=\u001b[39m l_args_4_7_ \u001b[38;5;241m=\u001b[39m l_args_4_8_ \u001b[38;5;241m=\u001b[39m l_args_4_9_ \u001b[38;5;241m=\u001b[39m mask_fn_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m getitem \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     20\u001b[0m getitem_1 \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m1\u001b[39m];  flex_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:97\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[1;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     86\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     mask_mod_other_buffers: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (),\n\u001b[0;32m     95\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m     96\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:524\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m    521\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    522\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:520\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    517\u001b[0m     )\n\u001b[0;32m    519\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m    521\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:380\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[1;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:781\u001b[0m, in \u001b[0;36mflex_attention_autograd\u001b[1;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m         fw_graph, bw_graph \u001b[38;5;241m=\u001b[39m score_mod, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m \u001b[43mFlexAttentionAutogradOp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\autograd\\function.py:576\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:646\u001b[0m, in \u001b[0;36mFlexAttentionAutogradOp.forward\u001b[1;34m(ctx, query, key, value, fw_graph, joint_graph, block_mask, scale, kernel_options, mask_mod_other_buffers, *score_mod_other_buffers)\u001b[0m\n\u001b[0;32m    644\u001b[0m ctx\u001b[38;5;241m.\u001b[39m_score_mod_other_buffers_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(score_mod_other_buffers)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_AutoDispatchBelowAutograd():\n\u001b[1;32m--> 646\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m \u001b[43mflex_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m save_tensors_and_symints_for_backward(\n\u001b[0;32m    659\u001b[0m     ctx,\n\u001b[0;32m    660\u001b[0m     (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m     ),\n\u001b[0;32m    670\u001b[0m )\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:97\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[1;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     86\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     mask_mod_other_buffers: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (),\n\u001b[0;32m     95\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m     96\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:524\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m    521\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    522\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:515\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    513\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m _to_flat_tuple(args, kwargs)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function(flat_args):\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    517\u001b[0m     )\n\u001b[0;32m    519\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m    521\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\overrides.py:1725\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[0;32m   1722\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[1;32m-> 1725\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1726\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1727\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_dynamo\\_trace_wrapped_higher_order_op.py:142\u001b[0m, in \u001b[0;36mTransformGetItemToIndex.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m index_args):\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mod_index(args[\u001b[38;5;241m0\u001b[39m], index_args)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:97\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[1;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     86\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     mask_mod_other_buffers: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (),\n\u001b[0;32m     95\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m     96\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:524\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m    521\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    522\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:520\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    517\u001b[0m     )\n\u001b[0;32m    519\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m    521\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_ops.py:380\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[1;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:271\u001b[0m, in \u001b[0;36msdpa_dense\u001b[1;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;129m@flex_attention\u001b[39m\u001b[38;5;241m.\u001b[39mpy_impl(DispatchKey\u001b[38;5;241m.\u001b[39mCompositeExplicitAutograd)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msdpa_dense\u001b[39m(\n\u001b[0;32m    261\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     mask_mod_other_buffers: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (),\n\u001b[0;32m    270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 271\u001b[0m     out, lse \u001b[38;5;241m=\u001b[39m \u001b[43mmath_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     out \u001b[38;5;241m=\u001b[39m _permute_strides(out, query\u001b[38;5;241m.\u001b[39mstride())\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:237\u001b[0m, in \u001b[0;36mmath_attention\u001b[1;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[0;32m    234\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mexpand((Bq, \u001b[38;5;241m*\u001b[39mkey\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m:]))\n\u001b[0;32m    235\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mexpand((Bq, \u001b[38;5;241m*\u001b[39mvalue\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m--> 237\u001b[0m _, post_mod_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_math_attention_inner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_mod_other_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Set fully masked rows' sumexp to 0.0\u001b[39;00m\n\u001b[0;32m    250\u001b[0m logsumexp \u001b[38;5;241m=\u001b[39m post_mod_scores\u001b[38;5;241m.\u001b[39mlogsumexp(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_higher_order_ops\\flex_attention.py:195\u001b[0m, in \u001b[0;36m_math_attention_inner\u001b[1;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TransformGetItemToIndex():\n\u001b[0;32m    191\u001b[0m     scores \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m*\u001b[39m scale)\u001b[38;5;241m.\u001b[39mto(working_precision)\n\u001b[0;32m    192\u001b[0m     post_mod_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m    193\u001b[0m         mask_mod(b, h, m, n, \u001b[38;5;241m*\u001b[39mmask_mod_other_buffers),\n\u001b[0;32m    194\u001b[0m         score_mod(scores, b, h, m, n, \u001b[38;5;241m*\u001b[39mscore_mod_other_buffers),\n\u001b[1;32m--> 195\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworking_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores, post_mod_scores\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\torch\\_dynamo\\_trace_wrapped_higher_order_op.py:142\u001b[0m, in \u001b[0;36mTransformGetItemToIndex.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m index_args):\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mod_index(args[\u001b[38;5;241m0\u001b[39m], index_args)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "bf16_ok = torch.cuda.is_bf16_supported()\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # токенизация и батчинг\n",
    "    max_seq_length=MAX_LEN,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    packing=PACKING,\n",
    "\n",
    "    # обучение\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # лог/сейвы\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # precision\n",
    "    bf16=bf16_ok,\n",
    "    fp16=not bf16_ok,\n",
    "\n",
    "    # критично для Windows\n",
    "    dataset_num_proc=1,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=train_ds,\n",
    "    # при желании добавь валидацию:\n",
    "    # eval_dataset=val_ds,  # и тогда ещё sft_args.eval_strategy=\"steps\"\n",
    "    processing_class=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved adapter to:\", OUTPUT_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27bc501",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "peft.peft_model.PeftModelForCausalLM.generate() argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(words, tags))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Quick smoke test:\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mКола Zero 1.5 литра\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTYPE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBRAND\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVOLUME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPERCENT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m, in \u001b[0;36mner_predict\u001b[1;34m(text, labels)\u001b[0m\n\u001b[0;32m     18\u001b[0m inp \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(msgs, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m                                     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minp,\n\u001b[0;32m     23\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m8\u001b[39m, n\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m     24\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     25\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m     26\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m resp \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(out[\u001b[38;5;241m0\u001b[39m][inp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     29\u001b[0m tags \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[1;31mTypeError\u001b[0m: peft.peft_model.PeftModelForCausalLM.generate() argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Reload labels detected during data prep\n",
    "# with open(\"entity_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     ENTITY_LABELS = json.load(f)\n",
    "\n",
    "def ner_predict(text: str, labels=None):\n",
    "    labels = labels\n",
    "    n = len(text.split(\" \"))\n",
    "    dev = {\"role\":\"developer\",\"content\":(\n",
    "        f\"You are an NER tagger. Use BIO with labels: {', '.join(labels)}. \"\n",
    "        f\"Return exactly {n} tags separated by a single space — one per whitespace-separated token. \"\n",
    "        \"Allowed: O, B-<LABEL>, I-<LABEL>. Do not add or remove tokens.\"\n",
    "    )}\n",
    "    usr = {\"role\":\"user\",\"content\": text}\n",
    "    msgs = [dev, usr]\n",
    "\n",
    "    inp = tokenizer.apply_chat_template(msgs, add_generation_prompt=True,\n",
    "                                        tokenize=True, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inp,\n",
    "            max_new_tokens=max(8, n*3),\n",
    "            do_sample=False, temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    resp = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip().splitlines()[0].strip()\n",
    "    tags = resp.split()\n",
    "    words = text.split(\" \")\n",
    "    if len(tags) < len(words): tags += [\"O\"] * (len(words) - len(tags))\n",
    "    if len(tags) > len(words): tags = tags[:len(words)]\n",
    "    return list(zip(words, tags))\n",
    "\n",
    "# Quick smoke test:\n",
    "ner_predict(\"Кола Zero 1.5 литра\", labels=['TYPE', 'BRAND', 'VOLUME', 'PERCENT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0372c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# Rebuild a tiny DataFrame from val_raw jsonl if needed:\n",
    "val_texts, val_tags = [], []\n",
    "import json as _json\n",
    "with open(VAL_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = _json.loads(line)\n",
    "        val_texts.append(obj[\"messages\"][1][\"content\"])\n",
    "        val_tags.append(obj[\"messages\"][2][\"content\"])\n",
    "val_frame = pd.DataFrame({\"sample\": val_texts, \"tags\": val_tags})\n",
    "\n",
    "def quick_eval(val_df: pd.DataFrame, limit: int = 200):\n",
    "    y_true, y_pred = [], []\n",
    "    for _, r in val_df.head(limit).iterrows():\n",
    "        gold = r[\"tags\"].split()\n",
    "        pred = [t for _, t in ner_predict(r[\"sample\"])]\n",
    "        y_true.append(gold); y_pred.append(pred)\n",
    "    print(\"F1:\", f1_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# quick_eval(val_frame, limit=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c529fc",
   "metadata": {},
   "source": [
    "### Из чекпоинта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb87577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile disabled & caches cleaned. CUDA bf16: True\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, glob, pathlib, platform, torch\n",
    "\n",
    "# 1) Жёстко отключаем всякую компиляцию/автотюн (важно сделать ДО импорта unsloth)\n",
    "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"          # отключить torch.compile внутри Unsloth\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"]   = \"1\"          # отключить inductor целиком\n",
    "os.environ[\"TRITON_AUTOTUNE\"]         = \"0\"          # отключить autotune в Triton\n",
    "os.environ[\"PYTORCH_TRITON_DISABLE_AUTOTUNE\"] = \"1\"  # то же\n",
    "\n",
    "# (опционально) короткие пути кешей, чтобы точно не упереться в странности путей Windows\n",
    "if platform.system() == \"Windows\":\n",
    "    os.environ.setdefault(\"TORCHINDUCTOR_CACHE_DIR\", r\"C:\\ti_cache\")\n",
    "    os.environ.setdefault(\"TRITON_CACHE_DIR\",       r\"C:\\triton_cache\")\n",
    "    pathlib.Path(os.environ[\"TORCHINDUCTOR_CACHE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(os.environ[\"TRITON_CACHE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Чистим предыдущие скомпилированные артефакты (важно, иначе поднимется старый compiled-модуль)\n",
    "shutil.rmtree(\"unsloth_compiled_cache\", ignore_errors=True)\n",
    "\n",
    "# Иногда полезно подчистить временные кеши torchinductor из %TEMP%\n",
    "for p in glob.glob(str(pathlib.Path.home() / \"AppData/Local/Temp/torchinductor_*\")):\n",
    "    shutil.rmtree(p, ignore_errors=True)\n",
    "\n",
    "# 3) На всякий случай отключим Dynamo \"на уровне процесса\"\n",
    "try:\n",
    "    import torch._dynamo as dynamo\n",
    "    dynamo.reset()\n",
    "    dynamo.disable()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Compile disabled & caches cleaned. CUDA bf16:\", torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc2ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xad in position 7: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 11:52:19.589000 4184 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Using checkpoint: gptoss20b_ner_bio_unsloth\\checkpoint-500\n",
      "==((====))==  Unsloth 2025.9.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready. Labels: ['BRAND', 'PERCENT', 'TYPE', 'VOLUME']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, json, pathlib\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "OUTPUT_DIR = \"gptoss20b_ner_bio_unsloth\"  # папка, где лежат checkpoint-XXXX\n",
    "MAX_LEN = 1024\n",
    "\n",
    "# 1) выбираем последний чекпоинт\n",
    "ckpts = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint-*\")),\n",
    "               key=lambda p: int(p.rsplit(\"-\", 1)[-1]))\n",
    "assert ckpts, f\"В {OUTPUT_DIR} нет checkpoint-*\"\n",
    "CHECKPOINT_DIR = ckpts[-1]\n",
    "print(\"Using checkpoint:\", CHECKPOINT_DIR)\n",
    "\n",
    "# 2) базовая модель (4-бит), линейризованная под Unsloth\n",
    "base, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gpt-oss-20b\",\n",
    "    load_in_4bit=True,\n",
    "    max_seq_length=MAX_LEN,\n",
    "    dtype=None,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3) подключаем адаптер из чекпоинта\n",
    "model = PeftModel.from_pretrained(base, CHECKPOINT_DIR)\n",
    "model.eval()\n",
    "\n",
    "# (на всякий случай) пад-токен\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# список меток сущностей\n",
    "with open(\"entity_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ENTITY_LABELS = json.load(f)\n",
    "\n",
    "print(\"Model ready. Labels:\", ENTITY_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4b4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 131072, 'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Кола', 'finalB-TYPE'), ('Zero', 'I-TYPE'), ('1.5', 'O'), ('литра', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import re, torch\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "def _as_int(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, (list, tuple)): x = x[0]\n",
    "    return int(x)\n",
    "\n",
    "def _safe_gen_config(model, tokenizer, max_new_tokens: int):\n",
    "    # создаём «чистую» конфигурацию генерации и заполняем руками\n",
    "    gc = GenerationConfig()\n",
    "    eos_id = _as_int(tokenizer.eos_token_id)\n",
    "    pad_id = _as_int(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id)\n",
    "    bos_id = _as_int(tokenizer.bos_token_id)\n",
    "    gc.eos_token_id = eos_id\n",
    "    gc.pad_token_id = pad_id\n",
    "    gc.bos_token_id = bos_id\n",
    "    gc.max_new_tokens = int(max_new_tokens)\n",
    "    gc.do_sample = False\n",
    "    gc.temperature = None\n",
    "    gc.num_beams = 1\n",
    "    gc.use_cache = True\n",
    "    return gc\n",
    "\n",
    "@torch.inference_mode()\n",
    "def ner_predict(text: str, labels=None):\n",
    "    labels = labels or ENTITY_LABELS\n",
    "\n",
    "    # нормализация пробелов к одному пробелу\n",
    "    norm_text = re.sub(r\"[ ]+\", \" \", text.strip())\n",
    "    n = len(norm_text.split(\" \"))\n",
    "\n",
    "    dev = {\"role\": \"developer\", \"content\": (\n",
    "        f\"You are an NER tagger. Use BIO with labels: {', '.join(labels)}. \"\n",
    "        f\"Return exactly {n} tags separated by a single space — one per whitespace-separated token. \"\n",
    "        \"Allowed: O, B-<LABEL>, I-<LABEL>. Do not add or remove tokens.\"\n",
    "    )}\n",
    "    usr = {\"role\": \"user\", \"content\": norm_text}\n",
    "    messages = [dev, usr]\n",
    "\n",
    "    # 1) текст промпта по Harmony-шаблону\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "    # 2) токенизация в mapping и перенос на девайс\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device).long() for k, v in inputs.items()}\n",
    "\n",
    "    # 3) «безопасная» конфигурация генерации\n",
    "    gen_cfg = _safe_gen_config(model, tokenizer, max_new_tokens=max(8, n * 3))\n",
    "\n",
    "    # 4) ВАЖНО: принудительно выключаем any-compile во время generate\n",
    "    try:\n",
    "        import torch._dynamo as dynamo\n",
    "        with dynamo.config.patch(cache_size_limit=0), torch._dynamo.disable():\n",
    "            gen_ids = model.generate(**inputs, generation_config=gen_cfg)\n",
    "    except Exception:\n",
    "        # если модуля нет/сломался патч — просто без него\n",
    "        gen_ids = model.generate(**inputs, generation_config=gen_cfg)\n",
    "\n",
    "    # 5) декод и выравнивание длины\n",
    "    out_text = tokenizer.decode(\n",
    "        gen_ids[0, inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip().splitlines()[0].strip()\n",
    "\n",
    "    tags = out_text.split()\n",
    "    words = norm_text.split(\" \")\n",
    "    if len(tags) < len(words): tags += [\"O\"] * (len(words) - len(tags))\n",
    "    if len(tags) > len(words): tags = tags[:len(words)]\n",
    "    return list(zip(words, tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6a7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('яерноголовка', 'finalяерноголовка'), ('1.5л', 'O')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# smoke-test\n",
    "print(ner_predict(\"яерноголовка 1.5л\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
