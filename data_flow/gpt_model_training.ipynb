{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26c247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_CSV = ../datasets/train_raw.csv\n",
      "bf16 supported: True\n"
     ]
    }
   ],
   "source": [
    "import os, platform, pathlib, torch, random, numpy as np\n",
    "\n",
    "# --- Windows-safe env flags ---\n",
    "if platform.system() == \"Windows\":\n",
    "    os.environ[\"HF_DATASETS_DISABLE_MULTIPROCESSING\"] = \"1\"  # –±–µ–∑ —Å–∞–±–ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤ datasets\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"           # –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞\n",
    "    os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"              # <--- –û–¢–ö–õ–Æ–ß–ò–¢–¨ torch.compile –≤ Unsloth\n",
    "    os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = r\"C:\\ti_cache\"   # –∫–æ—Ä–æ—Ç–∫–∏–π –ø—É—Ç—å –¥–ª—è inductor\n",
    "    os.environ[\"TRITON_CACHE_DIR\"]       = r\"C:\\triton_cache\"\n",
    "    pathlib.Path(r\"C:\\ti_cache\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(r\"C:\\triton_cache\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths (we'll pick the first that exists)\n",
    "CANDIDATE_CSVS = [\n",
    "    \"/mnt/data/train_raw.csv\",\n",
    "    \"../datasets/train_raw.csv\",\n",
    "    \"train_raw.csv\",\n",
    "]\n",
    "DATA_CSV = next((p for p in CANDIDATE_CSVS if pathlib.Path(p).exists()), CANDIDATE_CSVS[-1])\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"gptoss20b_ner_bio_unsloth\"\n",
    "TRAIN_JSONL = \"train.jsonl\"\n",
    "VAL_JSONL   = \"val.jsonl\"\n",
    "\n",
    "# Training knobs\n",
    "VAL_FRAC = 0.10\n",
    "MAX_LEN  = 1024     # –µ—Å–ª–∏ –±—É–¥–µ—Ç OOM, —Å–Ω–∏–∑—å –¥–æ 768/512\n",
    "RANK     = 8\n",
    "PACKING  = False    # –Ω–∞ Win —Å–Ω–∞—á–∞–ª–∞ False\n",
    "GRAD_ACC = 16\n",
    "FORCE_REGEN = False\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"DATA_CSV =\", DATA_CSV)\n",
    "print(\"bf16 supported:\", torch.cuda.is_bf16_supported())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5b673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL already exist; skipping regen. Set FORCE_REGEN=True to rebuild.\n"
     ]
    }
   ],
   "source": [
    "import ast, json, math\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "def parse_ann_cell(cell) -> List[Tuple[int,int,str]]:\n",
    "    \"\"\"annotation: list[(start, end, tag)] ‚Äî safe parse\"\"\"\n",
    "    if isinstance(cell, (list, tuple)):\n",
    "        ann = cell\n",
    "    else:\n",
    "        ann = ast.literal_eval(str(cell))\n",
    "    out = []\n",
    "    for t in ann:\n",
    "        if not (isinstance(t, (list, tuple)) and len(t) == 3):\n",
    "            raise ValueError(f\"Not a 3-tuple: {t}\")\n",
    "        s, e, tag = t\n",
    "        out.append((int(s), int(e), str(tag)))\n",
    "    return out\n",
    "\n",
    "def word_spans_exact_spaces(text: str) -> List[Tuple[int,int]]:\n",
    "    \"\"\"Split by the exact space ' ' only (keeps indices).\"\"\"\n",
    "    spans = []\n",
    "    i, n = 0, len(text)\n",
    "    while i < n:\n",
    "        while i < n and text[i] == ' ':\n",
    "            i += 1\n",
    "        if i >= n: break\n",
    "        j = i\n",
    "        while j < n and text[j] != ' ':\n",
    "            j += 1\n",
    "        spans.append((i, j))  # [i, j)\n",
    "        i = j\n",
    "    return spans\n",
    "\n",
    "def make_jsonl_if_needed(csv_path: str, train_path: str, val_path: str, val_frac: float, force: bool=False):\n",
    "    if (not force) and pathlib.Path(train_path).exists() and pathlib.Path(val_path).exists():\n",
    "        print(\"JSONL already exist; skipping regen. Set FORCE_REGEN=True to rebuild.\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading CSV:\", csv_path)\n",
    "    df = pd.read_csv(csv_path, sep=None, engine=\"python\").dropna(subset=[\"sample\",\"annotation\"]).reset_index(drop=True)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    rows = []\n",
    "    bad_parse = 0\n",
    "    bad_len_or_span = 0\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        text = str(r[\"sample\"])\n",
    "        try:\n",
    "            ann = parse_ann_cell(r[\"annotation\"])\n",
    "        except Exception:\n",
    "            bad_parse += 1\n",
    "            continue\n",
    "\n",
    "        spans = word_spans_exact_spaces(text)\n",
    "        if len(ann) != len(spans):\n",
    "            bad_len_or_span += 1\n",
    "            continue\n",
    "\n",
    "        ok = all((s == ws and e == we) for (s, e, _), (ws, we) in zip(ann, spans))\n",
    "        if not ok:\n",
    "            bad_len_or_span += 1\n",
    "            continue\n",
    "\n",
    "        tags_str = \" \".join(tag for _, _, tag in ann)\n",
    "        rows.append({\"sample\": text, \"tags\": tags_str, \"ann_tuples\": ann})\n",
    "\n",
    "    df2 = pd.DataFrame(rows)\n",
    "    print(f\"kept {len(df2)} rows; dropped parse_errors={bad_parse}, span/len_mismatches={bad_len_or_span}\")\n",
    "    if len(df2) == 0:\n",
    "        raise RuntimeError(\"No valid rows after validation ‚Äî check indices/spaces.\")\n",
    "\n",
    "    # Entity label names (without 'O')\n",
    "    entity_labels = sorted({\n",
    "        tag.split(\"-\", 1)[-1]\n",
    "        for ann in df2[\"ann_tuples\"]\n",
    "        for (_, _, tag) in ann\n",
    "        if tag != \"O\"\n",
    "    })\n",
    "    print(\"entity_labels:\", entity_labels)\n",
    "\n",
    "    # Train/val split (safe)\n",
    "    if len(df2) == 1:\n",
    "        train_df, val_df = df2.copy(), df2.iloc[[]].copy()\n",
    "    else:\n",
    "        val_n = max(1, int(round(len(df2) * val_frac)))\n",
    "        val_n = min(val_n, len(df2) - 1)\n",
    "        val_df = df2.sample(n=val_n, random_state=SEED)\n",
    "        train_df = df2.drop(val_df.index).reset_index(drop=True)\n",
    "        val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "    print(f\"train={len(train_df)}, val={len(val_df)}\")\n",
    "\n",
    "    def to_messages(text: str, tags: str):\n",
    "        n = len(tags.split())\n",
    "        dev = {\"role\":\"developer\",\"content\":(\n",
    "            f\"You are an NER tagger. Use BIO with labels: {', '.join(entity_labels)}. \"\n",
    "            f\"Return exactly {n} tags separated by a single space ‚Äî one per whitespace-separated token. \"\n",
    "            \"Allowed: O, B-<LABEL>, I-<LABEL>. Do not add or remove tokens.\"\n",
    "        )}\n",
    "        usr = {\"role\":\"user\",\"content\": text}\n",
    "        asst = {\"role\":\"assistant\",\"content\": tags}\n",
    "        return {\"messages\":[dev, usr, asst]}\n",
    "\n",
    "    def dump_jsonl(df_in: pd.DataFrame, path: str):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, r in df_in.iterrows():\n",
    "                f.write(json.dumps(to_messages(r[\"sample\"], r[\"tags\"]), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    dump_jsonl(train_df, train_path)\n",
    "    dump_jsonl(val_df,   val_path)\n",
    "\n",
    "    # Save entity labels for later reuse (inference)\n",
    "    with open(\"entity_labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(entity_labels, f, ensure_ascii=False)\n",
    "\n",
    "    print(\"Wrote:\", train_path, \"and\", val_path)\n",
    "\n",
    "make_jsonl_if_needed(DATA_CSV, TRAIN_JSONL, VAL_JSONL, VAL_FRAC, force=FORCE_REGEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5a3867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24526/24526 [00:02<00:00, 11869.51 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2725/2725 [00:00<00:00, 11109.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None)}\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-09-27\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "Yo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok_for_render = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\", use_fast=True)\n",
    "\n",
    "def map_row_to_text(ex):\n",
    "    txt = tok_for_render.apply_chat_template(\n",
    "        ex[\"messages\"],\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return {\"text\": txt}\n",
    "\n",
    "features = Features({\"text\": Value(\"string\")})\n",
    "train_raw = load_dataset(\"json\", data_files=TRAIN_JSONL, split=\"train\")\n",
    "val_raw   = load_dataset(\"json\", data_files=VAL_JSONL,   split=\"train\")\n",
    "\n",
    "# Avoid stale cache; and don't use multiprocessing on Windows\n",
    "train_ds = train_raw.map(map_row_to_text, remove_columns=train_raw.column_names,\n",
    "                         features=features, load_from_cache_file=False)\n",
    "val_ds   = val_raw.map(map_row_to_text,   remove_columns=val_raw.column_names,\n",
    "                       features=features, load_from_cache_file=False)\n",
    "\n",
    "print(train_ds.features)\n",
    "print(train_ds[0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efc5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lexan\\AppData\\Local\\Temp\\ipykernel_25896\\2665159020.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "Exception in thread Thread-5 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\lexan\\miniconda3\\envs\\gpt310\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xad in position 7: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0927 18:13:33.274000 25896 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.64s/it]\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Model & tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Use Unsloth's linearized gpt-oss 20B for QLoRA that fits ‚âà14 GB\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    load_in_4bit = True,\n",
    "    max_seq_length = MAX_LEN,\n",
    "    dtype = None,            # auto (bf16 if supported)\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Insert LoRA into all linear layers (attn + experts)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,\n",
    "    lora_alpha = 2 * RANK,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules = \"all-linear\",\n",
    "    bias = \"none\",\n",
    ")\n",
    "\n",
    "print(\"Model & tokenizer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9420131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200017}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 24,526 | Num Epochs = 2 | Total steps = 3,066\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 3,981,312 of 20,918,738,496 (0.02% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='3066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  61/3066 33:46 < 28:40:25, 0.03 it/s, Epoch 0.04/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.620300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.081200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "bf16_ok = torch.cuda.is_bf16_supported()\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –±–∞—Ç—á–∏–Ω–≥\n",
    "    max_seq_length=MAX_LEN,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    packing=PACKING,\n",
    "\n",
    "    # –æ–±—É—á–µ–Ω–∏–µ\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # –ª–æ–≥/—Å–µ–π–≤—ã\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # precision\n",
    "    bf16=bf16_ok,\n",
    "    fp16=not bf16_ok,\n",
    "\n",
    "    # –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è Windows\n",
    "    dataset_num_proc=1,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=train_ds,\n",
    "    # –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –¥–æ–±–∞–≤—å –≤–∞–ª–∏–¥–∞—Ü–∏—é:\n",
    "    # eval_dataset=val_ds,  # –∏ —Ç–æ–≥–¥–∞ –µ—â—ë sft_args.eval_strategy=\"steps\"\n",
    "    processing_class=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved adapter to:\", OUTPUT_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Reload labels detected during data prep\n",
    "with open(\"entity_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ENTITY_LABELS = json.load(f)\n",
    "\n",
    "def ner_predict(text: str, labels=None):\n",
    "    labels = labels or ENTITY_LABELS\n",
    "    n = len(text.split(\" \"))\n",
    "    dev = {\"role\":\"developer\",\"content\":(\n",
    "        f\"You are an NER tagger. Use BIO with labels: {', '.join(labels)}. \"\n",
    "        f\"Return exactly {n} tags separated by a single space ‚Äî one per whitespace-separated token. \"\n",
    "        \"Allowed: O, B-<LABEL>, I-<LABEL>. Do not add or remove tokens.\"\n",
    "    )}\n",
    "    usr = {\"role\":\"user\",\"content\": text}\n",
    "    msgs = [dev, usr]\n",
    "\n",
    "    inp = tokenizer.apply_chat_template(msgs, add_generation_prompt=True,\n",
    "                                        tokenize=True, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inp,\n",
    "            max_new_tokens=max(8, n*3),\n",
    "            do_sample=False, temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    resp = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip().splitlines()[0].strip()\n",
    "    tags = resp.split()\n",
    "    words = text.split(\" \")\n",
    "    if len(tags) < len(words): tags += [\"O\"] * (len(words) - len(tags))\n",
    "    if len(tags) > len(words): tags = tags[:len(words)]\n",
    "    return list(zip(words, tags))\n",
    "\n",
    "# Quick smoke test:\n",
    "# ner_predict(\"–ö–æ–ª–∞ Zero 1.5 –ª–∏—Ç—Ä–∞\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0372c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# Rebuild a tiny DataFrame from val_raw jsonl if needed:\n",
    "val_texts, val_tags = [], []\n",
    "import json as _json\n",
    "with open(VAL_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = _json.loads(line)\n",
    "        val_texts.append(obj[\"messages\"][1][\"content\"])\n",
    "        val_tags.append(obj[\"messages\"][2][\"content\"])\n",
    "val_frame = pd.DataFrame({\"sample\": val_texts, \"tags\": val_tags})\n",
    "\n",
    "def quick_eval(val_df: pd.DataFrame, limit: int = 200):\n",
    "    y_true, y_pred = [], []\n",
    "    for _, r in val_df.head(limit).iterrows():\n",
    "        gold = r[\"tags\"].split()\n",
    "        pred = [t for _, t in ner_predict(r[\"sample\"])]\n",
    "        y_true.append(gold); y_pred.append(pred)\n",
    "    print(\"F1:\", f1_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# quick_eval(val_frame, limit=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
