{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88615744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 27249 rows; columns: ['sample', 'annotation']\n",
      "Rows with at least one entity: 26513\n",
      "[PERCENT] clusters: 19; unique norm-forms: 19; avg cluster size: 1.000\n",
      "[VOLUME] clusters: 34; unique norm-forms: 36; avg cluster size: 1.059\n",
      "[BRAND] clusters: 2051; unique norm-forms: 4205; avg cluster size: 2.050\n",
      "[TYPE] clusters: 4005; unique norm-forms: 15637; avg cluster size: 3.904\n",
      "Strict components: 3045; samples with strict label(s): 24498\n",
      "Initial split -> train: 23979, val: 3270 (0.120 val ratio)\n",
      "[LEAK CHECK] TYPE: leak=0 (train clusters=2801, val clusters=1204)\n",
      "BRAND leak clusters: before=434 -> after=191; moved_to_train=287, moved_to_val=119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# ========================\n",
    "# ПАРАМЕТРЫ\n",
    "# ========================\n",
    "\n",
    "CSV_PATH = \"../datasets/train_raw.csv\"      # входной файл\n",
    "SEP = \";\"                       # разделитель\n",
    "VAL_SIZE = 0.4                 # доля валидации\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Кластеризация похожих форм\n",
    "RATIO_THR = 0.7               # порог похожести SequenceMatcher (0..1)\n",
    "MIN_PREFIX = 4                  # минимальная длина префикс-совпадения\n",
    "BUCKET_PREFIX_LEN = 3          # сколько символов без пробелов берем в ключ \"корзины\" для ускорения\n",
    "\n",
    "# Какие метки считаем валидными сущностями (BIO-суффиксы)\n",
    "VALID_LABELS = {\"BRAND\", \"TYPE\", \"VOLUME\", \"PERCENT\"}\n",
    "\n",
    "# Для каких меток делаем строгую защиту от утечки (рекомендуется оставить только TYPE)\n",
    "STRICT_LABELS = {\"TYPE\"}\n",
    "\n",
    "# Пытаться дополнительно уменьшить утечки по BRAND,\n",
    "# перенося меньшую сторону перекрывающегося бренд-кластера ТОЛЬКО среди строк без TYPE.\n",
    "REDUCE_BRAND_LEAK = True\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ\n",
    "# ========================\n",
    "\n",
    "def parse_ann_cell(cell):\n",
    "    \"\"\"annotation -> список (start, end, 'TAG')\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(cell)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def strip_bio(tag):\n",
    "    \"\"\"'B-TYPE' -> 'TYPE'; 'O' -> 'O'\"\"\"\n",
    "    if not isinstance(tag, str):\n",
    "        return \"O\"\n",
    "    if \"-\" in tag:\n",
    "        _, suf = tag.split(\"-\", 1)\n",
    "        return suf or \"O\"\n",
    "    return tag\n",
    "\n",
    "rus_to_lat_map = str.maketrans({\"ё\": \"е\"})\n",
    "def normalize_token(t: str) -> str:\n",
    "    return t.strip()\n",
    "\n",
    "def normalize_entity(text: str) -> str:\n",
    "    return \" \".join(normalize_token(tok) for tok in text.split())\n",
    "\n",
    "def extract_entities_from_row(text: str, ann_list):\n",
    "    \"\"\"Вернёт [(entity_text, label_suf), ...] без 'O', только из VALID_LABELS.\"\"\"\n",
    "    ents = []\n",
    "    for (s, e, tag) in ann_list:\n",
    "        lab = strip_bio(tag)\n",
    "        if lab in VALID_LABELS:\n",
    "            ents.append((text[s:e], lab))\n",
    "    return ents\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, items):\n",
    "        self.parent = {x: x for x in items}\n",
    "        self.rank = {x: 0 for x in items}\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb:\n",
    "            return False\n",
    "        if self.rank[ra] < self.rank[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.parent[rb] = ra\n",
    "        if self.rank[ra] == self.rank[rb]:\n",
    "            self.rank[ra] += 1\n",
    "        return True\n",
    "\n",
    "def bucket_key(s: str) -> str:\n",
    "    return (s.replace(\" \", \"\"))[:BUCKET_PREFIX_LEN]\n",
    "\n",
    "def similar(a: str, b: str, ratio_thr=RATIO_THR, min_prefix=MIN_PREFIX) -> bool:\n",
    "    if a == b:\n",
    "        return True\n",
    "    # Префикс (для незаконченного ввода): \"стан\" vs \"станк/станки\"\n",
    "    if a.startswith(b) or b.startswith(a):\n",
    "        if min(len(a), len(b)) >= min_prefix:\n",
    "            return True\n",
    "    # Очень короткие строки — избегаем шумных склеек\n",
    "    if max(len(a), len(b)) <= 4:\n",
    "        return False\n",
    "    r = SequenceMatcher(None, a, b).ratio()\n",
    "    return r >= ratio_thr\n",
    "\n",
    "def cluster_entities(entities_set, ratio_thr=RATIO_THR, min_prefix=MIN_PREFIX):\n",
    "    \"\"\"Кластеризация похожих нормализованных форм внутри одной метки.\"\"\"\n",
    "    ents = sorted(list(entities_set))\n",
    "    uf = UnionFind(ents)\n",
    "    buckets = defaultdict(list)\n",
    "    for e in ents:\n",
    "        buckets[bucket_key(e)].append(e)\n",
    "    for key, lst in buckets.items():\n",
    "        n = len(lst)\n",
    "        if n <= 1:\n",
    "            continue\n",
    "        for i in range(n):\n",
    "            a = lst[i]\n",
    "            for j in range(i+1, n):\n",
    "                b = lst[j]\n",
    "                if similar(a, b, ratio_thr=ratio_thr, min_prefix=min_prefix):\n",
    "                    uf.union(a, b)\n",
    "    groups = defaultdict(list)\n",
    "    for e in ents:\n",
    "        groups[uf.find(e)].append(e)\n",
    "    return groups  # {root: [members,...]}\n",
    "\n",
    "def build_label_groups(entity_rows, label: str):\n",
    "    \"\"\"Вернёт:\n",
    "    - groups: {root: [members]} для данного label\n",
    "    - ent_to_cluster: {normalized_entity: cluster_id}\n",
    "    - per_sample_clusters: список множеств cluster_id для каждого сэмпла\n",
    "    \"\"\"\n",
    "    # собрать множество нормализованных форм для данного label\n",
    "    uniq = set()\n",
    "    for ents in entity_rows:\n",
    "        for e_text, lab in ents:\n",
    "            if lab == label:\n",
    "                n = normalize_entity(e_text)\n",
    "                if n:\n",
    "                    uniq.add(n)\n",
    "    groups = cluster_entities(uniq)\n",
    "    ent_to_cluster = {}\n",
    "    for root, members in groups.items():\n",
    "        cid = f\"{label}|{root}\"\n",
    "        for m in members:\n",
    "            ent_to_cluster[m] = cid\n",
    "\n",
    "    per_sample_clusters = []\n",
    "    for ents in entity_rows:\n",
    "        cset = set()\n",
    "        for e_text, lab in ents:\n",
    "            if lab == label:\n",
    "                n = normalize_entity(e_text)\n",
    "                if n in ent_to_cluster:\n",
    "                    cset.add(ent_to_cluster[n])\n",
    "        per_sample_clusters.append(cset)\n",
    "    return groups, ent_to_cluster, per_sample_clusters\n",
    "\n",
    "def compute_leakage(per_sample_clusters, train_idx, val_idx):\n",
    "    \"\"\"Подсчёт утечки на уровне КЛАСТЕРОВ: сколько cluster_id встречается и в train, и в val.\"\"\"\n",
    "    tr, vl = set(), set()\n",
    "    for i in train_idx:\n",
    "        tr.update(per_sample_clusters[i])\n",
    "    for i in val_idx:\n",
    "        vl.update(per_sample_clusters[i])\n",
    "    return len(tr & vl), len(tr), len(vl)\n",
    "\n",
    "# ========================\n",
    "# ЗАГРУЗКА\n",
    "# ========================\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, sep=SEP, encoding=\"utf-8\")\n",
    "print(f\"Loaded: {len(df)} rows; columns: {list(df.columns)}\")\n",
    "\n",
    "# извлечь [(text,label), ...] на строку\n",
    "entity_rows = []\n",
    "for i, row in df.iterrows():\n",
    "    ann = parse_ann_cell(row[\"annotation\"])\n",
    "    entity_rows.append(extract_entities_from_row(row[\"sample\"], ann))\n",
    "\n",
    "print(\"Rows with at least one entity:\", sum(1 for r in entity_rows if r))\n",
    "\n",
    "# ========================\n",
    "# КЛАСТЕРИЗАЦИЯ ПО ЛЕЙБЛАМ\n",
    "# ========================\n",
    "\n",
    "label_groups = {}              # label -> {root: [members]}\n",
    "label_ent_to_cluster = {}      # label -> {norm_ent: cluster_id}\n",
    "label_per_sample = {}          # label -> [set(cluster_id), ...]\n",
    "\n",
    "for lab in VALID_LABELS:\n",
    "    groups, e2c, per_sample = build_label_groups(entity_rows, lab)\n",
    "    label_groups[lab] = groups\n",
    "    label_ent_to_cluster[lab] = e2c\n",
    "    label_per_sample[lab] = per_sample\n",
    "    total_entities = sum(len(v) for v in groups.values())\n",
    "    print(f\"[{lab}] clusters: {len(groups)}; unique norm-forms: {total_entities}; \"\n",
    "          f\"avg cluster size: {total_entities/len(groups) if groups else 0:.3f}\")\n",
    "\n",
    "# ========================\n",
    "# ГРУППЫ ДЛЯ STRICT_LABELS (Union по совместной встречаемости)\n",
    "# ========================\n",
    "\n",
    "# Построим Union-Find по КЛАСТЕРАМ выбранных меток: если в одном сэмпле встречаются несколько кластеров метки,\n",
    "# объединяем их в одну компоненту. Так исключаем утечку на уровне кластеров.\n",
    "all_groups = set()\n",
    "for lab in STRICT_LABELS:\n",
    "    all_groups.update({cid for cset in label_per_sample[lab] for cid in cset})\n",
    "all_groups = sorted(list(all_groups))\n",
    "uf = UnionFind(all_groups)\n",
    "\n",
    "for lab in STRICT_LABELS:\n",
    "    per_sample = label_per_sample[lab]\n",
    "    for cset in per_sample:\n",
    "        if len(cset) > 1:\n",
    "            cset = list(cset)\n",
    "            base = cset[0]\n",
    "            for other in cset[1:]:\n",
    "                uf.union(base, other)\n",
    "\n",
    "# Назначаем group-id для КАЖДОЙ СТРОКИ:\n",
    "# - если у строки есть кластеры любой strict-метки — gid = компонент id (find) одного из кластеров\n",
    "# - иначе — уникальный NO-STRICT|index (чтобы свободно распределялись и помогали балансировать долю)\n",
    "groups_for_samples = []\n",
    "strict_present = set()\n",
    "for idx in range(len(df)):\n",
    "    cands = set()\n",
    "    for lab in STRICT_LABELS:\n",
    "        cands.update(label_per_sample[lab][idx])\n",
    "    if cands:\n",
    "        any_cluster = next(iter(cands))\n",
    "        gid = uf.find(any_cluster)\n",
    "        groups_for_samples.append(gid)\n",
    "        strict_present.add(gid)\n",
    "    else:\n",
    "        groups_for_samples.append(f\"NO-STRICT|{idx}\")\n",
    "\n",
    "print(f\"Strict components: {len(strict_present)}; \"\n",
    "      f\"samples with strict label(s): {sum(1 for g in groups_for_samples if not str(g).startswith('NO-STRICT'))}\")\n",
    "\n",
    "# ========================\n",
    "# ГРУППОВОЙ СПЛИТ\n",
    "# ========================\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=RANDOM_STATE)\n",
    "idx_all = np.arange(len(df))\n",
    "train_idx, val_idx = next(gss.split(X=idx_all, groups=np.array(groups_for_samples)))\n",
    "\n",
    "print(f\"Initial split -> train: {len(train_idx)}, val: {len(val_idx)} \"\n",
    "      f\"({len(val_idx)/len(df):.3f} val ratio)\")\n",
    "\n",
    "# Проверим утечки по STRICT_LABELS (должно быть 0)\n",
    "for lab in STRICT_LABELS:\n",
    "    leak, tr_c, vl_c = compute_leakage(label_per_sample[lab], train_idx, val_idx)\n",
    "    print(f\"[LEAK CHECK] {lab}: leak={leak} (train clusters={tr_c}, val clusters={vl_c})\")\n",
    "\n",
    "# ========================\n",
    "# МЯГКАЯ МИНИМИЗАЦИЯ УТЕЧЕК ПО BRAND (не ломая TYPE)\n",
    "# ========================\n",
    "\n",
    "if REDUCE_BRAND_LEAK and \"BRAND\" in VALID_LABELS:\n",
    "    # 1) посчитаем утечки по BRAND\n",
    "    brand_leak_before, br_tr, br_vl = compute_leakage(label_per_sample[\"BRAND\"], train_idx, val_idx)\n",
    "\n",
    "    # Соберём, где какой бренд-кластер встречается\n",
    "    train_set = set(map(int, train_idx))\n",
    "    val_set = set(map(int, val_idx))\n",
    "\n",
    "    # Сопоставление \"строка -> множество бренд-кластеров\"\n",
    "    brand_per_sample = label_per_sample[\"BRAND\"]\n",
    "    type_per_sample = label_per_sample[\"TYPE\"] if \"TYPE\" in VALID_LABELS else [set()]*len(df)\n",
    "\n",
    "    # Соберём бренды по сторонам\n",
    "    train_brand_clusters, val_brand_clusters = defaultdict(list), defaultdict(list)\n",
    "    for i in range(len(df)):\n",
    "        if not brand_per_sample[i]:\n",
    "            continue\n",
    "        key = tuple(sorted(brand_per_sample[i]))  # допускаем несколько брендов в строке\n",
    "        if i in train_set:\n",
    "            train_brand_clusters[key].append(i)\n",
    "        elif i in val_set:\n",
    "            val_brand_clusters[key].append(i)\n",
    "\n",
    "    # Найдём пересечения (бренд-кластер(ы) в обеих выборках)\n",
    "    brand_overlap_keys = set(train_brand_clusters.keys()) & set(val_brand_clusters.keys())\n",
    "\n",
    "    # План переносов: переносим меньшую сторону ТОЛЬКО если у строк НЕТ TYPE\n",
    "    train_mask = np.zeros(len(df), dtype=bool)\n",
    "    train_mask[train_idx] = True\n",
    "    moved_to_train = moved_to_val = 0\n",
    "\n",
    "    for key in brand_overlap_keys:\n",
    "        tr_inds = train_brand_clusters[key]\n",
    "        vl_inds = val_brand_clusters[key]\n",
    "        # меньшая сторона\n",
    "        src_is_train = len(tr_inds) <= len(vl_inds)\n",
    "        src_inds = tr_inds if src_is_train else vl_inds\n",
    "        # проверим, что все переносимые индексы действительно без TYPE\n",
    "        src_inds_no_type = [i for i in src_inds if not type_per_sample[i]]\n",
    "        if len(src_inds_no_type) != len(src_inds):\n",
    "            # есть TYPE — не трогаем, чтобы не сломать гарантию\n",
    "            continue\n",
    "        # переносим\n",
    "        if src_is_train:\n",
    "            train_mask[np.array(src_inds_no_type, dtype=int)] = False\n",
    "            moved_to_val += len(src_inds_no_type)\n",
    "        else:\n",
    "            train_mask[np.array(src_inds_no_type, dtype=int)] = True\n",
    "            moved_to_train += len(src_inds_no_type)\n",
    "\n",
    "    train_idx = np.where(train_mask)[0]\n",
    "    val_idx = np.where(~train_mask)[0]\n",
    "\n",
    "    brand_leak_after, _, _ = compute_leakage(label_per_sample[\"BRAND\"], train_idx, val_idx)\n",
    "    print(f\"BRAND leak clusters: before={brand_leak_before} -> after={brand_leak_after}; \"\n",
    "          f\"moved_to_train={moved_to_train}, moved_to_val={moved_to_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655406fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train=24147, val=3102\n"
     ]
    }
   ],
   "source": [
    "# выбрать доступные индексы\n",
    "idx_train = train_idx if 'train_idx2' in globals() else train_idx\n",
    "idx_val   = val_idx   if 'val_idx2'   in globals() else val_idx\n",
    "\n",
    "# сформировать выборки\n",
    "train_out = df.iloc[idx_train].copy()\n",
    "val_out   = df.iloc[idx_val].copy()\n",
    "\n",
    "# сохранить\n",
    "train_out.to_csv(\"../datasets/train_split.csv\", sep=\";\", index=False, encoding=\"utf-8\")\n",
    "val_out.to_csv(\"../datasets/val_split.csv\",   sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved: train={len(train_out)}, val={len(val_out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67125e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк без TYPE: 2751 из 27249\n",
      "Train: 21959  Val: 5290  (val ~ 0.194)\n",
      "Пересечение TYPE-кластеров между train/val: 0\n",
      "Сохранено: train_split.csv (21959), val_split.csv (5290)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============ ПАРАМЕТРЫ ============\n",
    "CSV_PATH      = \"../datasets/train_raw.csv\"   # путь к твоему csv\n",
    "SEP           = \";\"                         # разделитель\n",
    "VAL_RATIO     = 0.2                         # доля валидации\n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "# Параметры похожести (n-граммы + Jaccard)\n",
    "NGRAM_N       = 3       # длина шингла (2..4)\n",
    "JACCARD_THR   = 0.69    # порог схожести (0.55..0.70)\n",
    "MIN_SHARED    = 3       # мин. общих n-грамм, чтобы проверять пару\n",
    "MIN_LEN       = 2       # игнорировать очень короткие строки\n",
    "USE_PREFIX    = True    # быстрый префикс-матч для обрезков\n",
    "MIN_PREFIX    = 3\n",
    "\n",
    "# Сохранение (можно выключить)\n",
    "SAVE_SPLIT    = True\n",
    "OUT_TRAIN     = \"../datasets/train_split.csv\"\n",
    "OUT_VAL       = \"../datatets/val_split.csv\"\n",
    "\n",
    "# ============ ЗАГРУЗКА ============\n",
    "df = pd.read_csv(CSV_PATH, sep=SEP, encoding=\"utf-8\")\n",
    "assert {\"sample\", \"annotation\"}.issubset(df.columns), \"Нужны колонки: sample, annotation\"\n",
    "\n",
    "# ============ ПАРСИНГ РАЗМЕТКИ ============\n",
    "def parse_ann(cell):\n",
    "    \"\"\"Строка вида \"[(0, 4, 'B-TYPE'), ...]\" -> список кортежей.\"\"\"\n",
    "    try:\n",
    "        lst = ast.literal_eval(cell)\n",
    "        if isinstance(lst, list):\n",
    "            return lst\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def strip_bio(tag):\n",
    "    \"\"\"B-TYPE -> TYPE, I-BRAND -> BRAND, 'O' -> 'O'.\"\"\"\n",
    "    if not isinstance(tag, str):\n",
    "        return \"O\"\n",
    "    if tag == \"O\":\n",
    "        return \"O\"\n",
    "    if \"-\" in tag:\n",
    "        _, suf = tag.split(\"-\", 1)\n",
    "        return suf\n",
    "    return tag\n",
    "\n",
    "def merge_entities(text, ann_list):\n",
    "    \"\"\"\n",
    "    Склеиваем соседние спаны одной метки (B/I) в цельные сущности.\n",
    "    ann_list: [(start, end, tag), ...] по символам.\n",
    "    \"\"\"\n",
    "    # отсортировать по start\n",
    "    spans = []\n",
    "    for s, e, t in ann_list:\n",
    "        t2 = strip_bio(t)\n",
    "        spans.append((int(s), int(e), t2))\n",
    "    spans.sort(key=lambda x: x[0])\n",
    "\n",
    "    merged = []\n",
    "    cur_s = cur_e = None\n",
    "    cur_lab = None\n",
    "\n",
    "    for s, e, lab in spans:\n",
    "        if lab == \"O\":\n",
    "            # закрыть текущую сущность, если была\n",
    "            if cur_lab is not None:\n",
    "                merged.append((cur_s, cur_e, cur_lab))\n",
    "                cur_s = cur_e = cur_lab = None\n",
    "            continue\n",
    "\n",
    "        if cur_lab is None:\n",
    "            cur_s, cur_e, cur_lab = s, e, lab\n",
    "        else:\n",
    "            if lab == cur_lab and (s <= cur_e + 1):  # допускаем пробел\n",
    "                cur_e = max(cur_e, e)\n",
    "            else:\n",
    "                merged.append((cur_s, cur_e, cur_lab))\n",
    "                cur_s, cur_e, cur_lab = s, e, lab\n",
    "\n",
    "    if cur_lab is not None:\n",
    "        merged.append((cur_s, cur_e, cur_lab))\n",
    "    # вернем (entity_text, label)\n",
    "    out = []\n",
    "    for s, e, lab in merged:\n",
    "        seg = text[s:e]\n",
    "        out.append((seg, lab))\n",
    "    return out\n",
    "\n",
    "# ============ НОРМАЛИЗАЦИЯ ============\n",
    "rus_to_lat_map = str.maketrans({\"ё\": \"е\"})\n",
    "\n",
    "def normalize_token(t: str) -> str:\n",
    "    t = t.lower().translate(rus_to_lat_map)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^0-9a-zа-я% ]+\", \"\", t)  # оставим буквы/цифры/процент/пробел\n",
    "    return t.strip()\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return \" \".join(normalize_token(tok) for tok in s.split())\n",
    "\n",
    "# ============ ИЗВЛЕЧЕНИЕ TYPE-СИГНАТУРЫ ДЛЯ КАЖДОЙ СТРОКИ ============\n",
    "# signature_type[i] — нормализованный текст всех TYPE-сущностей строки (склеенных),\n",
    "# если нет TYPE — None\n",
    "\n",
    "signature_type = []\n",
    "all_type_terms = set()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    text = str(row[\"sample\"])\n",
    "    ann  = parse_ann(row[\"annotation\"])\n",
    "    ents = merge_entities(text, ann)\n",
    "    type_pieces = []\n",
    "    for seg, lab in ents:\n",
    "        if lab == \"TYPE\":\n",
    "            type_pieces.append(normalize_text(seg))\n",
    "    if type_pieces:\n",
    "        sig = \" \".join(type_pieces)            # можно sorted(set(...)), если хочется уникальности\n",
    "        sig = re.sub(r\"\\s+\", \" \", sig).strip()\n",
    "        signature_type.append(sig if sig else None)\n",
    "        if sig:\n",
    "            all_type_terms.add(sig)\n",
    "    else:\n",
    "        signature_type.append(None)\n",
    "\n",
    "# ============ КЛАСТЕРИЗАЦИЯ TYPE ПО N-ГРАММАМ (Jaccard) ============\n",
    "def ngrams(s: str, n: int) -> set:\n",
    "    s2 = f\"^{s}$\"\n",
    "    return {s2[i:i+n] for i in range(max(0, len(s2) - n + 1))}\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    inter = len(a & b)\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    return inter / len(a | b)\n",
    "\n",
    "class UF:\n",
    "    def __init__(self, xs):\n",
    "        self.p = {x: x for x in xs}\n",
    "        self.r = {x: 0 for x in xs}\n",
    "    def f(self, x):\n",
    "        if self.p[x] != x:\n",
    "            self.p[x] = self.f(self.p[x])\n",
    "        return self.p[x]\n",
    "    def u(self, a, b):\n",
    "        ra, rb = self.f(a), self.f(b)\n",
    "        if ra == rb: return False\n",
    "        if self.r[ra] < self.r[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.p[rb] = ra\n",
    "        if self.r[ra] == self.r[rb]:\n",
    "            self.r[ra] += 1\n",
    "        return True\n",
    "\n",
    "# подготовим шинглы и инвертированный индекс\n",
    "terms = sorted([t for t in all_type_terms if len(t) >= MIN_LEN])\n",
    "shing = {t: ngrams(t, NGRAM_N) for t in terms}\n",
    "\n",
    "inv = defaultdict(list)\n",
    "for t, ss in shing.items():\n",
    "    for g in ss:\n",
    "        inv[g].append(t)\n",
    "\n",
    "uf = UF(terms)\n",
    "\n",
    "# подберём кандидатов: считаем число общих шинглов, фильтруем по MIN_SHARED, затем Jaccard\n",
    "for t in terms:\n",
    "    cand_counts = defaultdict(int)\n",
    "    ss = shing[t]\n",
    "    for g in ss:\n",
    "        for other in inv[g]:\n",
    "            if other == t:\n",
    "                continue\n",
    "            cand_counts[other] += 1\n",
    "\n",
    "    for other, c in cand_counts.items():\n",
    "        if c < MIN_SHARED:\n",
    "            continue\n",
    "\n",
    "        # префиксный матч для обрезков\n",
    "        if USE_PREFIX and (t.startswith(other) or other.startswith(t)):\n",
    "            if min(len(t), len(other)) >= MIN_PREFIX:\n",
    "                uf.u(t, other)\n",
    "                continue\n",
    "\n",
    "        # финальный Jaccard\n",
    "        if jaccard(ss, shing[other]) >= JACCARD_THR:\n",
    "            uf.u(t, other)\n",
    "\n",
    "# соберём кластеры TYPE: term -> cluster_id\n",
    "type_cluster_id = {}\n",
    "root_to_terms = defaultdict(list)\n",
    "for t in terms:\n",
    "    r = uf.f(t)\n",
    "    root_to_terms[r].append(t)\n",
    "for root, members in root_to_terms.items():\n",
    "    cid = f\"TYPE|{root}\"\n",
    "    for m in members:\n",
    "        type_cluster_id[m] = cid\n",
    "\n",
    "# ============ ГРУППЫ ДЛЯ СПЛИТА ============\n",
    "# группа строки = ее TYPE-кластер; если TYPE нет — уникальная группа на строку\n",
    "groups = []\n",
    "no_type_count = 0\n",
    "for i, sig in enumerate(signature_type):\n",
    "    if sig is None:\n",
    "        groups.append(f\"NO-TYPE|{i}\")\n",
    "        no_type_count += 1\n",
    "    else:\n",
    "        groups.append(type_cluster_id.get(sig, f\"TYPE-UNSEEN|{sig}\"))\n",
    "\n",
    "print(f\"Строк без TYPE: {no_type_count} из {len(df)}\")\n",
    "\n",
    "# ============ СПЛИТ ПО ГРУППАМ ============\n",
    "idx_all = np.arange(len(df))\n",
    "groups_arr = np.array(groups)\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=VAL_RATIO, random_state=RANDOM_STATE)\n",
    "    train_idx, val_idx = next(gss.split(idx_all, groups=groups_arr))\n",
    "except Exception as e:\n",
    "    # Fallback без sklearn: набираем целевые группы на валидацию\n",
    "    print(\"sklearn недоступен, используем резервный сплит. Ошибка:\", e)\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    uniq_groups = pd.unique(groups_arr)\n",
    "    rng.shuffle(uniq_groups)\n",
    "    counts = pd.Series(groups_arr).value_counts()\n",
    "    target_val = int(len(df) * VAL_RATIO)\n",
    "    picked, acc = [], 0\n",
    "    for g in uniq_groups:\n",
    "        c = int(counts[g])\n",
    "        if acc + c <= target_val:\n",
    "            picked.append(g)\n",
    "            acc += c\n",
    "        if acc >= target_val:\n",
    "            break\n",
    "    val_mask = np.isin(groups_arr, picked)\n",
    "    val_idx = np.where(val_mask)[0]\n",
    "    train_idx = np.where(~val_mask)[0]\n",
    "\n",
    "print(f\"Train: {len(train_idx)}  Val: {len(val_idx)}  (val ~ {len(val_idx)/len(df):.3f})\")\n",
    "\n",
    "# ============ ПРОВЕРКА ОТСУТСТВИЯ УТЕЧКИ ПО TYPE ============\n",
    "train_types = set()\n",
    "val_types   = set()\n",
    "for i in train_idx:\n",
    "    if signature_type[i] is not None:\n",
    "        train_types.add(type_cluster_id.get(signature_type[i], signature_type[i]))\n",
    "for i in val_idx:\n",
    "    if signature_type[i] is not None:\n",
    "        val_types.add(type_cluster_id.get(signature_type[i], signature_type[i]))\n",
    "\n",
    "leak = train_types & val_types\n",
    "print(f\"Пересечение TYPE-кластеров между train/val: {len(leak)}\")  # должно быть 0\n",
    "\n",
    "# ============ СОХРАНЕНИЕ ============\n",
    "if SAVE_SPLIT:\n",
    "    df.iloc[train_idx].to_csv(OUT_TRAIN, sep=SEP, index=False, encoding=\"utf-8\")\n",
    "    df.iloc[val_idx].to_csv(OUT_VAL,   sep=SEP, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Сохранено: {OUT_TRAIN} ({len(train_idx)}), {OUT_VAL} ({len(val_idx)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
