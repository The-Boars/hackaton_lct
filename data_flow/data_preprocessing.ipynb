{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88615744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 27251 rows; columns: ['sample', 'annotation']\n",
      "Rows with at least one entity: 26515\n",
      "[BRAND] clusters: 2052; unique norm-forms: 4206; avg cluster size: 2.050\n",
      "[VOLUME] clusters: 34; unique norm-forms: 36; avg cluster size: 1.059\n",
      "[TYPE] clusters: 4005; unique norm-forms: 15637; avg cluster size: 3.904\n",
      "[PERCENT] clusters: 19; unique norm-forms: 19; avg cluster size: 1.000\n",
      "Strict components: 3045; samples with strict label(s): 24499\n",
      "Initial split -> train: 1716, val: 25535 (0.937 val ratio)\n",
      "[LEAK CHECK] TYPE: leak=0 (train clusters=647, val clusters=3358)\n",
      "BRAND leak clusters: before=251 -> after=94; moved_to_train=10, moved_to_val=199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# ========================\n",
    "# ПАРАМЕТРЫ\n",
    "# ========================\n",
    "\n",
    "CSV_PATH = \"../datasets/train_raw.csv\"      # входной файл\n",
    "SEP = \";\"                       # разделитель\n",
    "VAL_SIZE = 0.8                 # доля валидации\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Кластеризация похожих форм\n",
    "RATIO_THR = 0.7               # порог похожести SequenceMatcher (0..1)\n",
    "MIN_PREFIX = 4                  # минимальная длина префикс-совпадения\n",
    "BUCKET_PREFIX_LEN = 3          # сколько символов без пробелов берем в ключ \"корзины\" для ускорения\n",
    "\n",
    "# Какие метки считаем валидными сущностями (BIO-суффиксы)\n",
    "VALID_LABELS = {\"BRAND\", \"TYPE\", \"VOLUME\", \"PERCENT\"}\n",
    "\n",
    "# Для каких меток делаем строгую защиту от утечки (рекомендуется оставить только TYPE)\n",
    "STRICT_LABELS = {\"TYPE\"}\n",
    "\n",
    "# Пытаться дополнительно уменьшить утечки по BRAND,\n",
    "# перенося меньшую сторону перекрывающегося бренд-кластера ТОЛЬКО среди строк без TYPE.\n",
    "REDUCE_BRAND_LEAK = True\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ\n",
    "# ========================\n",
    "\n",
    "def parse_ann_cell(cell):\n",
    "    \"\"\"annotation -> список (start, end, 'TAG')\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(cell)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def strip_bio(tag):\n",
    "    \"\"\"'B-TYPE' -> 'TYPE'; 'O' -> 'O'\"\"\"\n",
    "    if not isinstance(tag, str):\n",
    "        return \"O\"\n",
    "    if \"-\" in tag:\n",
    "        _, suf = tag.split(\"-\", 1)\n",
    "        return suf or \"O\"\n",
    "    return tag\n",
    "\n",
    "rus_to_lat_map = str.maketrans({\"ё\": \"е\"})\n",
    "def normalize_token(t: str) -> str:\n",
    "    return t.strip()\n",
    "\n",
    "def normalize_entity(text: str) -> str:\n",
    "    return \" \".join(normalize_token(tok) for tok in text.split())\n",
    "\n",
    "def extract_entities_from_row(text: str, ann_list):\n",
    "    \"\"\"Вернёт [(entity_text, label_suf), ...] без 'O', только из VALID_LABELS.\"\"\"\n",
    "    ents = []\n",
    "    for (s, e, tag) in ann_list:\n",
    "        lab = strip_bio(tag)\n",
    "        if lab in VALID_LABELS:\n",
    "            ents.append((text[s:e], lab))\n",
    "    return ents\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, items):\n",
    "        self.parent = {x: x for x in items}\n",
    "        self.rank = {x: 0 for x in items}\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb:\n",
    "            return False\n",
    "        if self.rank[ra] < self.rank[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.parent[rb] = ra\n",
    "        if self.rank[ra] == self.rank[rb]:\n",
    "            self.rank[ra] += 1\n",
    "        return True\n",
    "\n",
    "def bucket_key(s: str) -> str:\n",
    "    return (s.replace(\" \", \"\"))[:BUCKET_PREFIX_LEN]\n",
    "\n",
    "def similar(a: str, b: str, ratio_thr=RATIO_THR, min_prefix=MIN_PREFIX) -> bool:\n",
    "    if a == b:\n",
    "        return True\n",
    "    # Префикс (для незаконченного ввода): \"стан\" vs \"станк/станки\"\n",
    "    if a.startswith(b) or b.startswith(a):\n",
    "        if min(len(a), len(b)) >= min_prefix:\n",
    "            return True\n",
    "    # Очень короткие строки — избегаем шумных склеек\n",
    "    if max(len(a), len(b)) <= 4:\n",
    "        return False\n",
    "    r = SequenceMatcher(None, a, b).ratio()\n",
    "    return r >= ratio_thr\n",
    "\n",
    "def cluster_entities(entities_set, ratio_thr=RATIO_THR, min_prefix=MIN_PREFIX):\n",
    "    \"\"\"Кластеризация похожих нормализованных форм внутри одной метки.\"\"\"\n",
    "    ents = sorted(list(entities_set))\n",
    "    uf = UnionFind(ents)\n",
    "    buckets = defaultdict(list)\n",
    "    for e in ents:\n",
    "        buckets[bucket_key(e)].append(e)\n",
    "    for key, lst in buckets.items():\n",
    "        n = len(lst)\n",
    "        if n <= 1:\n",
    "            continue\n",
    "        for i in range(n):\n",
    "            a = lst[i]\n",
    "            for j in range(i+1, n):\n",
    "                b = lst[j]\n",
    "                if similar(a, b, ratio_thr=ratio_thr, min_prefix=min_prefix):\n",
    "                    uf.union(a, b)\n",
    "    groups = defaultdict(list)\n",
    "    for e in ents:\n",
    "        groups[uf.find(e)].append(e)\n",
    "    return groups  # {root: [members,...]}\n",
    "\n",
    "def build_label_groups(entity_rows, label: str):\n",
    "    \"\"\"Вернёт:\n",
    "    - groups: {root: [members]} для данного label\n",
    "    - ent_to_cluster: {normalized_entity: cluster_id}\n",
    "    - per_sample_clusters: список множеств cluster_id для каждого сэмпла\n",
    "    \"\"\"\n",
    "    # собрать множество нормализованных форм для данного label\n",
    "    uniq = set()\n",
    "    for ents in entity_rows:\n",
    "        for e_text, lab in ents:\n",
    "            if lab == label:\n",
    "                n = normalize_entity(e_text)\n",
    "                if n:\n",
    "                    uniq.add(n)\n",
    "    groups = cluster_entities(uniq)\n",
    "    ent_to_cluster = {}\n",
    "    for root, members in groups.items():\n",
    "        cid = f\"{label}|{root}\"\n",
    "        for m in members:\n",
    "            ent_to_cluster[m] = cid\n",
    "\n",
    "    per_sample_clusters = []\n",
    "    for ents in entity_rows:\n",
    "        cset = set()\n",
    "        for e_text, lab in ents:\n",
    "            if lab == label:\n",
    "                n = normalize_entity(e_text)\n",
    "                if n in ent_to_cluster:\n",
    "                    cset.add(ent_to_cluster[n])\n",
    "        per_sample_clusters.append(cset)\n",
    "    return groups, ent_to_cluster, per_sample_clusters\n",
    "\n",
    "def compute_leakage(per_sample_clusters, train_idx, val_idx):\n",
    "    \"\"\"Подсчёт утечки на уровне КЛАСТЕРОВ: сколько cluster_id встречается и в train, и в val.\"\"\"\n",
    "    tr, vl = set(), set()\n",
    "    for i in train_idx:\n",
    "        tr.update(per_sample_clusters[i])\n",
    "    for i in val_idx:\n",
    "        vl.update(per_sample_clusters[i])\n",
    "    return len(tr & vl), len(tr), len(vl)\n",
    "\n",
    "# ========================\n",
    "# ЗАГРУЗКА\n",
    "# ========================\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, sep=SEP, encoding=\"utf-8\")\n",
    "print(f\"Loaded: {len(df)} rows; columns: {list(df.columns)}\")\n",
    "\n",
    "# извлечь [(text,label), ...] на строку\n",
    "entity_rows = []\n",
    "for i, row in df.iterrows():\n",
    "    ann = parse_ann_cell(row[\"annotation\"])\n",
    "    entity_rows.append(extract_entities_from_row(row[\"sample\"], ann))\n",
    "\n",
    "print(\"Rows with at least one entity:\", sum(1 for r in entity_rows if r))\n",
    "\n",
    "# ========================\n",
    "# КЛАСТЕРИЗАЦИЯ ПО ЛЕЙБЛАМ\n",
    "# ========================\n",
    "\n",
    "label_groups = {}              # label -> {root: [members]}\n",
    "label_ent_to_cluster = {}      # label -> {norm_ent: cluster_id}\n",
    "label_per_sample = {}          # label -> [set(cluster_id), ...]\n",
    "\n",
    "for lab in VALID_LABELS:\n",
    "    groups, e2c, per_sample = build_label_groups(entity_rows, lab)\n",
    "    label_groups[lab] = groups\n",
    "    label_ent_to_cluster[lab] = e2c\n",
    "    label_per_sample[lab] = per_sample\n",
    "    total_entities = sum(len(v) for v in groups.values())\n",
    "    print(f\"[{lab}] clusters: {len(groups)}; unique norm-forms: {total_entities}; \"\n",
    "          f\"avg cluster size: {total_entities/len(groups) if groups else 0:.3f}\")\n",
    "\n",
    "# ========================\n",
    "# ГРУППЫ ДЛЯ STRICT_LABELS (Union по совместной встречаемости)\n",
    "# ========================\n",
    "\n",
    "# Построим Union-Find по КЛАСТЕРАМ выбранных меток: если в одном сэмпле встречаются несколько кластеров метки,\n",
    "# объединяем их в одну компоненту. Так исключаем утечку на уровне кластеров.\n",
    "all_groups = set()\n",
    "for lab in STRICT_LABELS:\n",
    "    all_groups.update({cid for cset in label_per_sample[lab] for cid in cset})\n",
    "all_groups = sorted(list(all_groups))\n",
    "uf = UnionFind(all_groups)\n",
    "\n",
    "for lab in STRICT_LABELS:\n",
    "    per_sample = label_per_sample[lab]\n",
    "    for cset in per_sample:\n",
    "        if len(cset) > 1:\n",
    "            cset = list(cset)\n",
    "            base = cset[0]\n",
    "            for other in cset[1:]:\n",
    "                uf.union(base, other)\n",
    "\n",
    "# Назначаем group-id для КАЖДОЙ СТРОКИ:\n",
    "# - если у строки есть кластеры любой strict-метки — gid = компонент id (find) одного из кластеров\n",
    "# - иначе — уникальный NO-STRICT|index (чтобы свободно распределялись и помогали балансировать долю)\n",
    "groups_for_samples = []\n",
    "strict_present = set()\n",
    "for idx in range(len(df)):\n",
    "    cands = set()\n",
    "    for lab in STRICT_LABELS:\n",
    "        cands.update(label_per_sample[lab][idx])\n",
    "    if cands:\n",
    "        any_cluster = next(iter(cands))\n",
    "        gid = uf.find(any_cluster)\n",
    "        groups_for_samples.append(gid)\n",
    "        strict_present.add(gid)\n",
    "    else:\n",
    "        groups_for_samples.append(f\"NO-STRICT|{idx}\")\n",
    "\n",
    "print(f\"Strict components: {len(strict_present)}; \"\n",
    "      f\"samples with strict label(s): {sum(1 for g in groups_for_samples if not str(g).startswith('NO-STRICT'))}\")\n",
    "\n",
    "# ========================\n",
    "# ГРУППОВОЙ СПЛИТ\n",
    "# ========================\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=RANDOM_STATE)\n",
    "idx_all = np.arange(len(df))\n",
    "train_idx, val_idx = next(gss.split(X=idx_all, groups=np.array(groups_for_samples)))\n",
    "\n",
    "print(f\"Initial split -> train: {len(train_idx)}, val: {len(val_idx)} \"\n",
    "      f\"({len(val_idx)/len(df):.3f} val ratio)\")\n",
    "\n",
    "# Проверим утечки по STRICT_LABELS (должно быть 0)\n",
    "for lab in STRICT_LABELS:\n",
    "    leak, tr_c, vl_c = compute_leakage(label_per_sample[lab], train_idx, val_idx)\n",
    "    print(f\"[LEAK CHECK] {lab}: leak={leak} (train clusters={tr_c}, val clusters={vl_c})\")\n",
    "\n",
    "# ========================\n",
    "# МЯГКАЯ МИНИМИЗАЦИЯ УТЕЧЕК ПО BRAND (не ломая TYPE)\n",
    "# ========================\n",
    "\n",
    "if REDUCE_BRAND_LEAK and \"BRAND\" in VALID_LABELS:\n",
    "    # 1) посчитаем утечки по BRAND\n",
    "    brand_leak_before, br_tr, br_vl = compute_leakage(label_per_sample[\"BRAND\"], train_idx, val_idx)\n",
    "\n",
    "    # Соберём, где какой бренд-кластер встречается\n",
    "    train_set = set(map(int, train_idx))\n",
    "    val_set = set(map(int, val_idx))\n",
    "\n",
    "    # Сопоставление \"строка -> множество бренд-кластеров\"\n",
    "    brand_per_sample = label_per_sample[\"BRAND\"]\n",
    "    type_per_sample = label_per_sample[\"TYPE\"] if \"TYPE\" in VALID_LABELS else [set()]*len(df)\n",
    "\n",
    "    # Соберём бренды по сторонам\n",
    "    train_brand_clusters, val_brand_clusters = defaultdict(list), defaultdict(list)\n",
    "    for i in range(len(df)):\n",
    "        if not brand_per_sample[i]:\n",
    "            continue\n",
    "        key = tuple(sorted(brand_per_sample[i]))  # допускаем несколько брендов в строке\n",
    "        if i in train_set:\n",
    "            train_brand_clusters[key].append(i)\n",
    "        elif i in val_set:\n",
    "            val_brand_clusters[key].append(i)\n",
    "\n",
    "    # Найдём пересечения (бренд-кластер(ы) в обеих выборках)\n",
    "    brand_overlap_keys = set(train_brand_clusters.keys()) & set(val_brand_clusters.keys())\n",
    "\n",
    "    # План переносов: переносим меньшую сторону ТОЛЬКО если у строк НЕТ TYPE\n",
    "    train_mask = np.zeros(len(df), dtype=bool)\n",
    "    train_mask[train_idx] = True\n",
    "    moved_to_train = moved_to_val = 0\n",
    "\n",
    "    for key in brand_overlap_keys:\n",
    "        tr_inds = train_brand_clusters[key]\n",
    "        vl_inds = val_brand_clusters[key]\n",
    "        # меньшая сторона\n",
    "        src_is_train = len(tr_inds) <= len(vl_inds)\n",
    "        src_inds = tr_inds if src_is_train else vl_inds\n",
    "        # проверим, что все переносимые индексы действительно без TYPE\n",
    "        src_inds_no_type = [i for i in src_inds if not type_per_sample[i]]\n",
    "        if len(src_inds_no_type) != len(src_inds):\n",
    "            # есть TYPE — не трогаем, чтобы не сломать гарантию\n",
    "            continue\n",
    "        # переносим\n",
    "        if src_is_train:\n",
    "            train_mask[np.array(src_inds_no_type, dtype=int)] = False\n",
    "            moved_to_val += len(src_inds_no_type)\n",
    "        else:\n",
    "            train_mask[np.array(src_inds_no_type, dtype=int)] = True\n",
    "            moved_to_train += len(src_inds_no_type)\n",
    "\n",
    "    train_idx = np.where(train_mask)[0]\n",
    "    val_idx = np.where(~train_mask)[0]\n",
    "\n",
    "    brand_leak_after, _, _ = compute_leakage(label_per_sample[\"BRAND\"], train_idx, val_idx)\n",
    "    print(f\"BRAND leak clusters: before={brand_leak_before} -> after={brand_leak_after}; \"\n",
    "          f\"moved_to_train={moved_to_train}, moved_to_val={moved_to_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655406fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train=1527, val=25724\n"
     ]
    }
   ],
   "source": [
    "# выбрать доступные индексы\n",
    "idx_train = train_idx if 'train_idx2' in globals() else train_idx\n",
    "idx_val   = val_idx   if 'val_idx2'   in globals() else val_idx\n",
    "\n",
    "# сформировать выборки\n",
    "train_out = df.iloc[idx_train].copy()\n",
    "val_out   = df.iloc[idx_val].copy()\n",
    "\n",
    "# сохранить\n",
    "train_out.to_csv(\"../datasets/train_split.csv\", sep=\";\", index=False, encoding=\"utf-8\")\n",
    "val_out.to_csv(\"../datasets/val_split.csv\",   sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved: train={len(train_out)}, val={len(val_out)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
