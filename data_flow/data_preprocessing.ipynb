{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00292bf9",
   "metadata": {},
   "source": [
    "Предразбивка данных по группам на основании похожести"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67125e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк без TYPE: 2751 из 27249\n",
      "Train: 21959  Val: 5290  (val ~ 0.194)\n",
      "Пересечение TYPE-кластеров между train/val: 0\n",
      "Сохранено: train_split.csv (21959), val_split.csv (5290)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV_PATH      = \"../datasets/train_raw.csv\" \n",
    "SEP           = \";\"                         \n",
    "VAL_RATIO     = 0.2                         \n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "\n",
    "NGRAM_N       = 3      \n",
    "JACCARD_THR   = 0.69   \n",
    "MIN_SHARED    = 3      \n",
    "MIN_LEN       = 2      \n",
    "USE_PREFIX    = True   \n",
    "MIN_PREFIX    = 3\n",
    "\n",
    "SAVE_SPLIT    = True\n",
    "OUT_TRAIN     = \"../datasets/train_split.csv\"\n",
    "OUT_VAL       = \"../datatets/val_split.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, sep=SEP, encoding=\"utf-8\")\n",
    "assert {\"sample\", \"annotation\"}.issubset(df.columns), \"Нужны колонки: sample, annotation\"\n",
    "\n",
    "\n",
    "def parse_ann(cell):\n",
    "\n",
    "    try:\n",
    "        lst = ast.literal_eval(cell)\n",
    "        if isinstance(lst, list):\n",
    "            return lst\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def strip_bio(tag):\n",
    "    if not isinstance(tag, str):\n",
    "        return \"O\"\n",
    "    if tag == \"O\":\n",
    "        return \"O\"\n",
    "    if \"-\" in tag:\n",
    "        _, suf = tag.split(\"-\", 1)\n",
    "        return suf\n",
    "    return tag\n",
    "\n",
    "def merge_entities(text, ann_list):\n",
    "    spans = []\n",
    "    for s, e, t in ann_list:\n",
    "        t2 = strip_bio(t)\n",
    "        spans.append((int(s), int(e), t2))\n",
    "    spans.sort(key=lambda x: x[0])\n",
    "\n",
    "    merged = []\n",
    "    cur_s = cur_e = None\n",
    "    cur_lab = None\n",
    "\n",
    "    for s, e, lab in spans:\n",
    "        if lab == \"O\":\n",
    "            if cur_lab is not None:\n",
    "                merged.append((cur_s, cur_e, cur_lab))\n",
    "                cur_s = cur_e = cur_lab = None\n",
    "            continue\n",
    "\n",
    "        if cur_lab is None:\n",
    "            cur_s, cur_e, cur_lab = s, e, lab\n",
    "        else:\n",
    "            if lab == cur_lab and (s <= cur_e + 1):\n",
    "                cur_e = max(cur_e, e)\n",
    "            else:\n",
    "                merged.append((cur_s, cur_e, cur_lab))\n",
    "                cur_s, cur_e, cur_lab = s, e, lab\n",
    "\n",
    "    if cur_lab is not None:\n",
    "        merged.append((cur_s, cur_e, cur_lab))\n",
    "    out = []\n",
    "    for s, e, lab in merged:\n",
    "        seg = text[s:e]\n",
    "        out.append((seg, lab))\n",
    "    return out\n",
    "\n",
    "rus_to_lat_map = str.maketrans({\"ё\": \"е\"})\n",
    "\n",
    "def normalize_token(t: str) -> str:\n",
    "    t = t.lower().translate(rus_to_lat_map)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^0-9a-zа-я% ]+\", \"\", t) \n",
    "    return t.strip()\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return \" \".join(normalize_token(tok) for tok in s.split())\n",
    "\n",
    "\n",
    "signature_type = []\n",
    "all_type_terms = set()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    text = str(row[\"sample\"])\n",
    "    ann  = parse_ann(row[\"annotation\"])\n",
    "    ents = merge_entities(text, ann)\n",
    "    type_pieces = []\n",
    "    for seg, lab in ents:\n",
    "        if lab == \"TYPE\":\n",
    "            type_pieces.append(normalize_text(seg))\n",
    "    if type_pieces:\n",
    "        sig = \" \".join(type_pieces)            \n",
    "        sig = re.sub(r\"\\s+\", \" \", sig).strip()\n",
    "        signature_type.append(sig if sig else None)\n",
    "        if sig:\n",
    "            all_type_terms.add(sig)\n",
    "    else:\n",
    "        signature_type.append(None)\n",
    "\n",
    "def ngrams(s: str, n: int) -> set:\n",
    "    s2 = f\"^{s}$\"\n",
    "    return {s2[i:i+n] for i in range(max(0, len(s2) - n + 1))}\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    inter = len(a & b)\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    return inter / len(a | b)\n",
    "\n",
    "class UF:\n",
    "    def __init__(self, xs):\n",
    "        self.p = {x: x for x in xs}\n",
    "        self.r = {x: 0 for x in xs}\n",
    "    def f(self, x):\n",
    "        if self.p[x] != x:\n",
    "            self.p[x] = self.f(self.p[x])\n",
    "        return self.p[x]\n",
    "    def u(self, a, b):\n",
    "        ra, rb = self.f(a), self.f(b)\n",
    "        if ra == rb: return False\n",
    "        if self.r[ra] < self.r[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.p[rb] = ra\n",
    "        if self.r[ra] == self.r[rb]:\n",
    "            self.r[ra] += 1\n",
    "        return True\n",
    "\n",
    "terms = sorted([t for t in all_type_terms if len(t) >= MIN_LEN])\n",
    "shing = {t: ngrams(t, NGRAM_N) for t in terms}\n",
    "\n",
    "inv = defaultdict(list)\n",
    "for t, ss in shing.items():\n",
    "    for g in ss:\n",
    "        inv[g].append(t)\n",
    "\n",
    "uf = UF(terms)\n",
    "\n",
    "for t in terms:\n",
    "    cand_counts = defaultdict(int)\n",
    "    ss = shing[t]\n",
    "    for g in ss:\n",
    "        for other in inv[g]:\n",
    "            if other == t:\n",
    "                continue\n",
    "            cand_counts[other] += 1\n",
    "\n",
    "    for other, c in cand_counts.items():\n",
    "        if c < MIN_SHARED:\n",
    "            continue\n",
    "\n",
    "        if USE_PREFIX and (t.startswith(other) or other.startswith(t)):\n",
    "            if min(len(t), len(other)) >= MIN_PREFIX:\n",
    "                uf.u(t, other)\n",
    "                continue\n",
    "\n",
    "\n",
    "        if jaccard(ss, shing[other]) >= JACCARD_THR:\n",
    "            uf.u(t, other)\n",
    "\n",
    "\n",
    "type_cluster_id = {}\n",
    "root_to_terms = defaultdict(list)\n",
    "for t in terms:\n",
    "    r = uf.f(t)\n",
    "    root_to_terms[r].append(t)\n",
    "for root, members in root_to_terms.items():\n",
    "    cid = f\"TYPE|{root}\"\n",
    "    for m in members:\n",
    "        type_cluster_id[m] = cid\n",
    "\n",
    "groups = []\n",
    "no_type_count = 0\n",
    "for i, sig in enumerate(signature_type):\n",
    "    if sig is None:\n",
    "        groups.append(f\"NO-TYPE|{i}\")\n",
    "        no_type_count += 1\n",
    "    else:\n",
    "        groups.append(type_cluster_id.get(sig, f\"TYPE-UNSEEN|{sig}\"))\n",
    "\n",
    "print(f\"Строк без TYPE: {no_type_count} из {len(df)}\")\n",
    "\n",
    "idx_all = np.arange(len(df))\n",
    "groups_arr = np.array(groups)\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=VAL_RATIO, random_state=RANDOM_STATE)\n",
    "    train_idx, val_idx = next(gss.split(idx_all, groups=groups_arr))\n",
    "except Exception as e:\n",
    "    print(\"sklearn недоступен, используем резервный сплит. Ошибка:\", e)\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    uniq_groups = pd.unique(groups_arr)\n",
    "    rng.shuffle(uniq_groups)\n",
    "    counts = pd.Series(groups_arr).value_counts()\n",
    "    target_val = int(len(df) * VAL_RATIO)\n",
    "    picked, acc = [], 0\n",
    "    for g in uniq_groups:\n",
    "        c = int(counts[g])\n",
    "        if acc + c <= target_val:\n",
    "            picked.append(g)\n",
    "            acc += c\n",
    "        if acc >= target_val:\n",
    "            break\n",
    "    val_mask = np.isin(groups_arr, picked)\n",
    "    val_idx = np.where(val_mask)[0]\n",
    "    train_idx = np.where(~val_mask)[0]\n",
    "\n",
    "print(f\"Train: {len(train_idx)}  Val: {len(val_idx)}  (val ~ {len(val_idx)/len(df):.3f})\")\n",
    "\n",
    "\n",
    "train_types = set()\n",
    "val_types   = set()\n",
    "for i in train_idx:\n",
    "    if signature_type[i] is not None:\n",
    "        train_types.add(type_cluster_id.get(signature_type[i], signature_type[i]))\n",
    "for i in val_idx:\n",
    "    if signature_type[i] is not None:\n",
    "        val_types.add(type_cluster_id.get(signature_type[i], signature_type[i]))\n",
    "\n",
    "leak = train_types & val_types\n",
    "print(f\"Пересечение TYPE-кластеров между train/val: {len(leak)}\")  \n",
    "\n",
    "\n",
    "if SAVE_SPLIT:\n",
    "    df.iloc[train_idx].to_csv(OUT_TRAIN, sep=SEP, index=False, encoding=\"utf-8\")\n",
    "    df.iloc[val_idx].to_csv(OUT_VAL,   sep=SEP, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Сохранено: {OUT_TRAIN} ({len(train_idx)}), {OUT_VAL} ({len(val_idx)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
