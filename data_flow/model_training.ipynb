{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57024b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeeppavlov\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[1;32m----> 3\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner_collection3_bert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\deeppavlov\\core\\commands\\infer.py:37\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[0;32m     34\u001b[0m config \u001b[38;5;241m=\u001b[39m parse_config(config)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install:\n\u001b[1;32m---> 37\u001b[0m     \u001b[43minstall_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m     39\u001b[0m     deep_download(config)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\deeppavlov\\utils\\pip_wrapper\\pip_wrapper.py:71\u001b[0m, in \u001b[0;36minstall_from_config\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 requirements\u001b[38;5;241m.\u001b[39mappend(line)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m requirements:\n\u001b[1;32m---> 71\u001b[0m     \u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\deeppavlov\\utils\\pip_wrapper\\pip_wrapper.py:36\u001b[0m, in \u001b[0;36minstall\u001b[1;34m(*packages)\u001b[0m\n\u001b[0;32m     34\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound tensorflow-gpu installed, so upgrading it instead of tensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m     packages \u001b[38;5;241m=\u001b[39m [_tf_re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow-gpu\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, package) \u001b[38;5;28;01mfor\u001b[39;00m package \u001b[38;5;129;01min\u001b[39;00m packages]\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                               \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\subprocess.py:364\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_call\u001b[39m(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m call(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[0;32m    366\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\subprocess.py:347\u001b[0m, in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# Including KeyboardInterrupt, wait handled that.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m         p\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\subprocess.py:1506\u001b[0m, in \u001b[0;36mPopen._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1503\u001b[0m     timeout_millis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;66;03m# API note: Returns immediately if timeout_millis == 0.\u001b[39;00m\n\u001b[1;32m-> 1506\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForSingleObject\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtimeout_millis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWAIT_TIMEOUT:\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model('ner_collection3_bert', download=True, install=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebaef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.deeppavlov/downloads/collection3/\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import train_model, build_model \n",
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "PROJECT_DIR = '..'\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "model_config = parse_config('ner_collection3_bert')\n",
    "\n",
    "# dataset that the model was trained on\n",
    "print(model_config['dataset_reader']['data_path'])\n",
    "\n",
    "model_config['dataset_reader']['data_path'] = PROJECT_DIR + '/datasets/conll/'\n",
    "\n",
    "del model_config['metadata']['download']\n",
    "\n",
    "\n",
    "model_config['dataset_reader']['iobes'] = False\n",
    "model_config['metadata']['variables']['MODEL_PATH'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "\n",
    "model_config['chainer']['pipe'][1]['save_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "model_config['chainer']['pipe'][1]['load_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "\n",
    "model_config['chainer']['pipe'][2]['save_path'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "model_config['chainer']['pipe'][2]['load_path'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "\n",
    "\n",
    "model_config['train']['batch_size'] = 400\n",
    "\n",
    "model_config['train']['log_every_n_batches'] = 10\n",
    "model_config['train']['val_every_n_batches'] = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1a155a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_reader': {'class_name': 'conll2003_reader',\n",
       "  'data_path': '../datasets/conll/',\n",
       "  'dataset_name': 'collection3',\n",
       "  'provide_pos': False,\n",
       "  'provide_chunk': False,\n",
       "  'iobes': True},\n",
       " 'dataset_iterator': {'class_name': 'data_learning_iterator'},\n",
       " 'chainer': {'in': ['x'],\n",
       "  'in_y': ['y'],\n",
       "  'pipe': [{'class_name': 'torch_transformers_ner_preprocessor',\n",
       "    'vocab_file': 'DeepPavlov/rubert-base-cased',\n",
       "    'do_lower_case': False,\n",
       "    'max_seq_length': 512,\n",
       "    'max_subword_length': 15,\n",
       "    'token_masking_prob': 0.0,\n",
       "    'in': ['x'],\n",
       "    'out': ['x_tokens',\n",
       "     'x_subword_tokens',\n",
       "     'x_subword_tok_ids',\n",
       "     'startofword_markers',\n",
       "     'attention_mask',\n",
       "     'tokens_offsets']},\n",
       "   {'id': 'tag_vocab',\n",
       "    'class_name': 'simple_vocab',\n",
       "    'unk_token': ['O'],\n",
       "    'pad_with_zeros': True,\n",
       "    'save_path': '../models/tag.dict',\n",
       "    'load_path': '../models/tag.dict',\n",
       "    'fit_on': ['y'],\n",
       "    'in': ['y'],\n",
       "    'out': ['y_ind']},\n",
       "   {'class_name': 'torch_transformers_sequence_tagger',\n",
       "    'n_tags': '#tag_vocab.len',\n",
       "    'pretrained_bert': 'DeepPavlov/rubert-base-cased',\n",
       "    'attention_probs_keep_prob': 0.5,\n",
       "    'encoder_layer_ids': [-1],\n",
       "    'optimizer': 'AdamW',\n",
       "    'optimizer_parameters': {'lr': 2e-05,\n",
       "     'weight_decay': 1e-06,\n",
       "     'betas': [0.9, 0.999],\n",
       "     'eps': 1e-06},\n",
       "    'clip_norm': 1.0,\n",
       "    'min_learning_rate': 1e-07,\n",
       "    'learning_rate_drop_patience': 30,\n",
       "    'learning_rate_drop_div': 1.5,\n",
       "    'load_before_drop': True,\n",
       "    'save_path': '../models/model',\n",
       "    'load_path': '../models/model',\n",
       "    'in': ['x_subword_tok_ids', 'attention_mask', 'startofword_markers'],\n",
       "    'in_y': ['y_ind'],\n",
       "    'out': ['y_pred_ind', 'probas']},\n",
       "   {'ref': 'tag_vocab', 'in': ['y_pred_ind'], 'out': ['y_pred']}],\n",
       "  'out': ['x_tokens', 'y_pred']},\n",
       " 'train': {'epochs': 30,\n",
       "  'batch_size': 400,\n",
       "  'metrics': [{'name': 'ner_f1', 'inputs': ['y', 'y_pred']},\n",
       "   {'name': 'ner_token_f1', 'inputs': ['y', 'y_pred']}],\n",
       "  'validation_patience': 100,\n",
       "  'val_every_n_batches': 10,\n",
       "  'log_every_n_batches': 10,\n",
       "  'show_examples': False,\n",
       "  'pytest_max_batches': 2,\n",
       "  'pytest_batch_size': 8,\n",
       "  'evaluation_targets': ['valid', 'test'],\n",
       "  'class_name': 'torch_trainer'},\n",
       " 'metadata': {'variables': {'ROOT_PATH': '~/.deeppavlov',\n",
       "   'DOWNLOADS_PATH': '~/.deeppavlov/downloads',\n",
       "   'MODELS_PATH': '~/.deeppavlov/models',\n",
       "   'TRANSFORMER': 'DeepPavlov/rubert-base-cased',\n",
       "   'MODEL_PATH': '../models/model'},\n",
       "  'requirements': ['c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/torchcrf.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/protobuf.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/sentencepiece.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/pytorch.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/transformers.txt']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd27e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 12:24:56.61 WARNING in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 66: TorchTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-09-28 12:24:59.578 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 104: [saving vocabulary to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\tag.dict]\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-09-28 12:25:01.220 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 151: Init from scratch. Load path C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar does not exist.\n",
      "11it [00:01,  7.00it/s]\n",
      "2025-09-28 12:25:02.841 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 198: Initial best ner_f1 of 20.7359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 20.7359, \"ner_token_f1\": 37.0924}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 52.2622, \"ner_token_f1\": 70.4762}, \"time_spent\": \"0:00:04\", \"epochs_done\": 0, \"batches_seen\": 10, \"train_examples_seen\": 4000, \"loss\": 1.0764714688062669}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.73it/s]\n",
      "2025-09-28 12:25:05.673 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 20.7359 to 58.7745\n",
      "2025-09-28 12:25:05.674 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:05.674 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 58.7745, \"ner_token_f1\": 78.9528}, \"time_spent\": \"0:00:05\", \"epochs_done\": 0, \"batches_seen\": 10, \"train_examples_seen\": 4000, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 14.37it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 64.9573, \"ner_token_f1\": 79.3934}, \"time_spent\": \"0:00:10\", \"epochs_done\": 0, \"batches_seen\": 20, \"train_examples_seen\": 8000, \"loss\": 0.3729159340262413}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.86it/s]\n",
      "2025-09-28 12:25:11.305 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 58.7745 to 65.1256\n",
      "2025-09-28 12:25:11.305 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:11.306 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 65.1256, \"ner_token_f1\": 84.2926}, \"time_spent\": \"0:00:11\", \"epochs_done\": 0, \"batches_seen\": 20, \"train_examples_seen\": 8000, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.53it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 83.9045, \"ner_token_f1\": 89.1941}, \"time_spent\": \"0:00:16\", \"epochs_done\": 0, \"batches_seen\": 30, \"train_examples_seen\": 12000, \"loss\": 0.22165409922599794}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.47it/s]\n",
      "2025-09-28 12:25:17.120 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 65.1256 to 79.4544\n",
      "2025-09-28 12:25:17.121 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:17.122 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 79.4544, \"ner_token_f1\": 86.5205}, \"time_spent\": \"0:00:16\", \"epochs_done\": 0, \"batches_seen\": 30, \"train_examples_seen\": 12000, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.94it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 85.6263, \"ner_token_f1\": 87.9201}, \"time_spent\": \"0:00:21\", \"epochs_done\": 0, \"batches_seen\": 40, \"train_examples_seen\": 16000, \"loss\": 0.16577921360731124}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.00it/s]\n",
      "2025-09-28 12:25:22.881 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 79.4544 to 83.6585\n",
      "2025-09-28 12:25:22.882 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:22.884 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 83.6585, \"ner_token_f1\": 88.2449}, \"time_spent\": \"0:00:22\", \"epochs_done\": 0, \"batches_seen\": 40, \"train_examples_seen\": 16000, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:24,  1.97it/s]\n",
      "1it [00:00, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 87.0662, \"ner_token_f1\": 91.2656}, \"time_spent\": \"0:00:27\", \"epochs_done\": 1, \"batches_seen\": 50, \"train_examples_seen\": 19875, \"loss\": 0.13604694455862046}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.15it/s]\n",
      "2025-09-28 12:25:28.601 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 83.6585 to 86.0242\n",
      "2025-09-28 12:25:28.602 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:28.603 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 86.0242, \"ner_token_f1\": 90.2929}, \"time_spent\": \"0:00:28\", \"epochs_done\": 1, \"batches_seen\": 50, \"train_examples_seen\": 19875, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.25it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 88.4656, \"ner_token_f1\": 90.8248}, \"time_spent\": \"0:00:33\", \"epochs_done\": 1, \"batches_seen\": 60, \"train_examples_seen\": 23875, \"loss\": 0.10450552254915238}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.81it/s]\n",
      "2025-09-28 12:25:34.668 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 86.0242 to 87.1903\n",
      "2025-09-28 12:25:34.669 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:34.670 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 87.1903, \"ner_token_f1\": 91.1224}, \"time_spent\": \"0:00:34\", \"epochs_done\": 1, \"batches_seen\": 60, \"train_examples_seen\": 23875, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.35it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 89.9258, \"ner_token_f1\": 91.8002}, \"time_spent\": \"0:00:39\", \"epochs_done\": 1, \"batches_seen\": 70, \"train_examples_seen\": 27875, \"loss\": 0.10213656052947044}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.18it/s]\n",
      "2025-09-28 12:25:40.804 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 87.1903 to 87.7532\n",
      "2025-09-28 12:25:40.805 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:40.806 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 87.7532, \"ner_token_f1\": 91.4671}, \"time_spent\": \"0:00:40\", \"epochs_done\": 1, \"batches_seen\": 70, \"train_examples_seen\": 27875, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.91it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 90.2083, \"ner_token_f1\": 93.7611}, \"time_spent\": \"0:00:45\", \"epochs_done\": 1, \"batches_seen\": 80, \"train_examples_seen\": 31875, \"loss\": 0.09685129299759865}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.34it/s]\n",
      "2025-09-28 12:25:46.600 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 87.7532 to 88.2722\n",
      "2025-09-28 12:25:46.602 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:46.603 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.2722, \"ner_token_f1\": 91.9683}, \"time_spent\": \"0:00:46\", \"epochs_done\": 1, \"batches_seen\": 80, \"train_examples_seen\": 31875, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.03it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 92.229, \"ner_token_f1\": 94.4544}, \"time_spent\": \"0:00:51\", \"epochs_done\": 1, \"batches_seen\": 90, \"train_examples_seen\": 35875, \"loss\": 0.09036510959267616}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.60it/s]\n",
      "2025-09-28 12:25:52.505 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 88.2722 to 88.4562\n",
      "2025-09-28 12:25:52.506 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:25:52.507 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.4562, \"ner_token_f1\": 91.8805}, \"time_spent\": \"0:00:52\", \"epochs_done\": 1, \"batches_seen\": 90, \"train_examples_seen\": 35875, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:29,  1.62it/s]\n",
      "1it [00:00, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 91.2355, \"ner_token_f1\": 94.7955}, \"time_spent\": \"0:00:57\", \"epochs_done\": 2, \"batches_seen\": 100, \"train_examples_seen\": 39750, \"loss\": 0.08366802744567395}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.46it/s]\n",
      "2025-09-28 12:25:58.699 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 88.4562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 85.4184, \"ner_token_f1\": 89.9709}, \"time_spent\": \"0:00:58\", \"epochs_done\": 2, \"batches_seen\": 100, \"train_examples_seen\": 39750, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.69it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 91.1017, \"ner_token_f1\": 94.1721}, \"time_spent\": \"0:01:00\", \"epochs_done\": 2, \"batches_seen\": 110, \"train_examples_seen\": 43750, \"loss\": 0.07959185689687728}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.40it/s]\n",
      "2025-09-28 12:26:01.912 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 88.4562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 87.3527, \"ner_token_f1\": 91.2467}, \"time_spent\": \"0:01:01\", \"epochs_done\": 2, \"batches_seen\": 110, \"train_examples_seen\": 43750, \"impatience\": 2, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 10.12it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 94.9625, \"ner_token_f1\": 95.4802}, \"time_spent\": \"0:01:03\", \"epochs_done\": 2, \"batches_seen\": 120, \"train_examples_seen\": 47750, \"loss\": 0.07087178528308868}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.72it/s]\n",
      "2025-09-28 12:26:05.289 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 88.4562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.0238, \"ner_token_f1\": 91.5171}, \"time_spent\": \"0:01:05\", \"epochs_done\": 2, \"batches_seen\": 120, \"train_examples_seen\": 47750, \"impatience\": 3, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.32it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 92.6518, \"ner_token_f1\": 94.5622}, \"time_spent\": \"0:01:07\", \"epochs_done\": 2, \"batches_seen\": 130, \"train_examples_seen\": 51750, \"loss\": 0.07363534234464168}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.53it/s]\n",
      "2025-09-28 12:26:08.344 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 88.4562 to 88.8038\n",
      "2025-09-28 12:26:08.344 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:26:08.345 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.8038, \"ner_token_f1\": 92.124}, \"time_spent\": \"0:01:08\", \"epochs_done\": 2, \"batches_seen\": 130, \"train_examples_seen\": 51750, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.56it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 94.1922, \"ner_token_f1\": 95.0971}, \"time_spent\": \"0:01:13\", \"epochs_done\": 2, \"batches_seen\": 140, \"train_examples_seen\": 55750, \"loss\": 0.06734145805239677}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.30it/s]\n",
      "2025-09-28 12:26:14.677 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 88.8038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.6866, \"ner_token_f1\": 92.0051}, \"time_spent\": \"0:01:14\", \"epochs_done\": 2, \"batches_seen\": 140, \"train_examples_seen\": 55750, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:18,  2.58it/s]\n",
      "1it [00:00, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 94.8634, \"ner_token_f1\": 96.2099}, \"time_spent\": \"0:01:16\", \"epochs_done\": 3, \"batches_seen\": 150, \"train_examples_seen\": 59625, \"loss\": 0.05430237092077732}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.68it/s]\n",
      "2025-09-28 12:26:17.754 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 88.8038 to 89.8582\n",
      "2025-09-28 12:26:17.755 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:26:17.756 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.8582, \"ner_token_f1\": 92.4499}, \"time_spent\": \"0:01:17\", \"epochs_done\": 3, \"batches_seen\": 150, \"train_examples_seen\": 59625, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.17it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 93.4737, \"ner_token_f1\": 95.2909}, \"time_spent\": \"0:01:22\", \"epochs_done\": 3, \"batches_seen\": 160, \"train_examples_seen\": 63625, \"loss\": 0.057194629684090614}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.66it/s]\n",
      "2025-09-28 12:26:23.952 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 89.8582 to 89.8845\n",
      "2025-09-28 12:26:23.953 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:26:23.953 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.8845, \"ner_token_f1\": 92.5428}, \"time_spent\": \"0:01:23\", \"epochs_done\": 3, \"batches_seen\": 160, \"train_examples_seen\": 63625, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.33it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.5454, \"ner_token_f1\": 97.9323}, \"time_spent\": \"0:01:28\", \"epochs_done\": 3, \"batches_seen\": 170, \"train_examples_seen\": 67625, \"loss\": 0.055471161380410194}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.80it/s]\n",
      "2025-09-28 12:26:29.990 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 89.8845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.9687, \"ner_token_f1\": 92.176}, \"time_spent\": \"0:01:29\", \"epochs_done\": 3, \"batches_seen\": 170, \"train_examples_seen\": 67625, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.46it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 96.2884, \"ner_token_f1\": 96.913}, \"time_spent\": \"0:01:31\", \"epochs_done\": 3, \"batches_seen\": 180, \"train_examples_seen\": 71625, \"loss\": 0.05656243562698364}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:01, 10.99it/s]\n",
      "2025-09-28 12:26:33.44 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 89.8845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.7461, \"ner_token_f1\": 91.7173}, \"time_spent\": \"0:01:32\", \"epochs_done\": 3, \"batches_seen\": 180, \"train_examples_seen\": 71625, \"impatience\": 2, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.65it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 95.4447, \"ner_token_f1\": 96.9245}, \"time_spent\": \"0:01:34\", \"epochs_done\": 3, \"batches_seen\": 190, \"train_examples_seen\": 75625, \"loss\": 0.048584960773587225}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.82it/s]\n",
      "2025-09-28 12:26:35.946 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 89.8845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.8415, \"ner_token_f1\": 91.7284}, \"time_spent\": \"0:01:35\", \"epochs_done\": 3, \"batches_seen\": 190, \"train_examples_seen\": 75625, \"impatience\": 3, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:20,  2.31it/s]\n",
      "1it [00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 96.162, \"ner_token_f1\": 97.0315}, \"time_spent\": \"0:01:37\", \"epochs_done\": 4, \"batches_seen\": 200, \"train_examples_seen\": 79500, \"loss\": 0.04674988854676485}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.56it/s]\n",
      "2025-09-28 12:26:38.778 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 89.8845 to 90.0853\n",
      "2025-09-28 12:26:38.778 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:26:38.779 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.0853, \"ner_token_f1\": 92.6589}, \"time_spent\": \"0:01:38\", \"epochs_done\": 4, \"batches_seen\": 200, \"train_examples_seen\": 79500, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.54it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 96.8008, \"ner_token_f1\": 97.786}, \"time_spent\": \"0:01:48\", \"epochs_done\": 4, \"batches_seen\": 210, \"train_examples_seen\": 83500, \"loss\": 0.03801875524222851}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.12it/s]\n",
      "2025-09-28 12:26:49.533 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.0853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.7319, \"ner_token_f1\": 92.6027}, \"time_spent\": \"0:01:49\", \"epochs_done\": 4, \"batches_seen\": 210, \"train_examples_seen\": 83500, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.05it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 96.3124, \"ner_token_f1\": 97.277}, \"time_spent\": \"0:01:51\", \"epochs_done\": 4, \"batches_seen\": 220, \"train_examples_seen\": 87500, \"loss\": 0.04263503365218639}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.91it/s]\n",
      "2025-09-28 12:26:52.489 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.0853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.6795, \"ner_token_f1\": 92.4309}, \"time_spent\": \"0:01:52\", \"epochs_done\": 4, \"batches_seen\": 220, \"train_examples_seen\": 87500, \"impatience\": 2, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.12it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.9167, \"ner_token_f1\": 98.3825}, \"time_spent\": \"0:01:54\", \"epochs_done\": 4, \"batches_seen\": 230, \"train_examples_seen\": 91500, \"loss\": 0.04708518050611019}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.79it/s]\n",
      "2025-09-28 12:26:55.381 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 90.0853 to 90.5773\n",
      "2025-09-28 12:26:55.382 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:26:55.383 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.5773, \"ner_token_f1\": 92.9893}, \"time_spent\": \"0:01:55\", \"epochs_done\": 4, \"batches_seen\": 230, \"train_examples_seen\": 91500, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.99it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.1246, \"ner_token_f1\": 97.6614}, \"time_spent\": \"0:02:00\", \"epochs_done\": 4, \"batches_seen\": 240, \"train_examples_seen\": 95375, \"loss\": 0.04463544897735119}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.50it/s]\n",
      "2025-09-28 12:27:01.401 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.5359, \"ner_token_f1\": 93.0059}, \"time_spent\": \"0:02:01\", \"epochs_done\": 4, \"batches_seen\": 240, \"train_examples_seen\": 95375, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:25,  1.91it/s]\n",
      "1it [00:00, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 98.0769, \"ner_token_f1\": 98.3114}, \"time_spent\": \"0:02:03\", \"epochs_done\": 5, \"batches_seen\": 250, \"train_examples_seen\": 99375, \"loss\": 0.036909295991063115}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.94it/s]\n",
      "2025-09-28 12:27:04.359 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.7781, \"ner_token_f1\": 92.3965}, \"time_spent\": \"0:02:04\", \"epochs_done\": 5, \"batches_seen\": 250, \"train_examples_seen\": 99375, \"impatience\": 2, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.94it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.7395, \"ner_token_f1\": 98.0132}, \"time_spent\": \"0:02:06\", \"epochs_done\": 5, \"batches_seen\": 260, \"train_examples_seen\": 103375, \"loss\": 0.03663092106580734}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.67it/s]\n",
      "2025-09-28 12:27:07.347 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.9746, \"ner_token_f1\": 91.6674}, \"time_spent\": \"0:02:07\", \"epochs_done\": 5, \"batches_seen\": 260, \"train_examples_seen\": 103375, \"impatience\": 3, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.47it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 98.2942, \"ner_token_f1\": 98.2955}, \"time_spent\": \"0:02:09\", \"epochs_done\": 5, \"batches_seen\": 270, \"train_examples_seen\": 107375, \"loss\": 0.031686913967132566}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.02it/s]\n",
      "2025-09-28 12:27:10.180 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.5916, \"ner_token_f1\": 92.1736}, \"time_spent\": \"0:02:09\", \"epochs_done\": 5, \"batches_seen\": 270, \"train_examples_seen\": 107375, \"impatience\": 4, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 13.38it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.2103, \"ner_token_f1\": 98.4274}, \"time_spent\": \"0:02:11\", \"epochs_done\": 5, \"batches_seen\": 280, \"train_examples_seen\": 111375, \"loss\": 0.03236878495663405}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 13.49it/s]\n",
      "2025-09-28 12:27:12.988 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.5717, \"ner_token_f1\": 92.2754}, \"time_spent\": \"0:02:12\", \"epochs_done\": 5, \"batches_seen\": 280, \"train_examples_seen\": 111375, \"impatience\": 5, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:12,  3.69it/s]\n",
      "1it [00:00, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 98.4547, \"ner_token_f1\": 98.6486}, \"time_spent\": \"0:02:14\", \"epochs_done\": 6, \"batches_seen\": 290, \"train_examples_seen\": 115250, \"loss\": 0.032039379328489305}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 11.41it/s]\n",
      "2025-09-28 12:27:15.847 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.3748, \"ner_token_f1\": 92.8057}, \"time_spent\": \"0:02:15\", \"epochs_done\": 6, \"batches_seen\": 290, \"train_examples_seen\": 115250, \"impatience\": 6, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 13.41it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.4576, \"ner_token_f1\": 98.3333}, \"time_spent\": \"0:02:17\", \"epochs_done\": 6, \"batches_seen\": 300, \"train_examples_seen\": 119250, \"loss\": 0.02933723609894514}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.55it/s]\n",
      "2025-09-28 12:27:18.736 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.0564, \"ner_token_f1\": 92.5173}, \"time_spent\": \"0:02:18\", \"epochs_done\": 6, \"batches_seen\": 300, \"train_examples_seen\": 119250, \"impatience\": 7, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.27it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.234, \"ner_token_f1\": 98.0574}, \"time_spent\": \"0:02:20\", \"epochs_done\": 6, \"batches_seen\": 310, \"train_examples_seen\": 123250, \"loss\": 0.025816111639142037}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.91it/s]\n",
      "2025-09-28 12:27:21.675 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.5647, \"ner_token_f1\": 91.4489}, \"time_spent\": \"0:02:21\", \"epochs_done\": 6, \"batches_seen\": 310, \"train_examples_seen\": 123250, \"impatience\": 8, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.12it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 96.9506, \"ner_token_f1\": 97.3272}, \"time_spent\": \"0:02:23\", \"epochs_done\": 6, \"batches_seen\": 320, \"train_examples_seen\": 127250, \"loss\": 0.03048244435340166}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.76it/s]\n",
      "2025-09-28 12:27:24.619 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 90.5773 to 90.8588\n",
      "2025-09-28 12:27:24.619 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-28 12:27:24.620 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.8588, \"ner_token_f1\": 93.1907}, \"time_spent\": \"0:02:24\", \"epochs_done\": 6, \"batches_seen\": 320, \"train_examples_seen\": 127250, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 12.72it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 97.5558, \"ner_token_f1\": 98.0207}, \"time_spent\": \"0:02:29\", \"epochs_done\": 6, \"batches_seen\": 330, \"train_examples_seen\": 131250, \"loss\": 0.03020241567865014}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.95it/s]\n",
      "2025-09-28 12:27:30.449 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.6357, \"ner_token_f1\": 92.9107}, \"time_spent\": \"0:02:30\", \"epochs_done\": 6, \"batches_seen\": 330, \"train_examples_seen\": 131250, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:17,  2.80it/s]\n",
      "1it [00:00, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 400, \"metrics\": {\"ner_f1\": 98.7315, \"ner_token_f1\": 99.2551}, \"time_spent\": \"0:02:32\", \"epochs_done\": 7, \"batches_seen\": 340, \"train_examples_seen\": 135125, \"loss\": 0.02390593271702528}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:01, 10.80it/s]\n",
      "2025-09-28 12:27:33.458 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 90.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 88.8913, \"ner_token_f1\": 91.6163}, \"time_spent\": \"0:02:33\", \"epochs_done\": 7, \"batches_seen\": 340, \"train_examples_seen\": 135125, \"impatience\": 2, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:02,  2.93it/s]\n",
      "2025-09-28 12:27:34.245 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 336: Stopped training\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11it [00:01, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 90.8588, \"ner_token_f1\": 93.1907}, \"time_spent\": \"0:00:02\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:00, 12.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"test\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 89.5144, \"ner_token_f1\": 93.0765}, \"time_spent\": \"0:00:01\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ner_model = train_model(model_config, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25a8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_reader': {'class_name': 'conll2003_reader',\n",
       "  'data_path': '../datasets/conll/',\n",
       "  'dataset_name': 'collection3',\n",
       "  'provide_pos': False,\n",
       "  'provide_chunk': False,\n",
       "  'iobes': True},\n",
       " 'dataset_iterator': {'class_name': 'data_learning_iterator'},\n",
       " 'chainer': {'in': ['x'],\n",
       "  'in_y': ['y'],\n",
       "  'pipe': [{'class_name': 'torch_transformers_ner_preprocessor',\n",
       "    'vocab_file': 'DeepPavlov/rubert-base-cased',\n",
       "    'do_lower_case': False,\n",
       "    'max_seq_length': 512,\n",
       "    'max_subword_length': 15,\n",
       "    'token_masking_prob': 0.0,\n",
       "    'in': ['x'],\n",
       "    'out': ['x_tokens',\n",
       "     'x_subword_tokens',\n",
       "     'x_subword_tok_ids',\n",
       "     'startofword_markers',\n",
       "     'attention_mask',\n",
       "     'tokens_offsets']},\n",
       "   {'id': 'tag_vocab',\n",
       "    'class_name': 'simple_vocab',\n",
       "    'unk_token': ['O'],\n",
       "    'pad_with_zeros': True,\n",
       "    'save_path': '../models/tag.dict',\n",
       "    'load_path': '../models/tag.dict',\n",
       "    'fit_on': ['y'],\n",
       "    'in': ['y'],\n",
       "    'out': ['y_ind']},\n",
       "   {'class_name': 'torch_transformers_sequence_tagger',\n",
       "    'n_tags': '#tag_vocab.len',\n",
       "    'pretrained_bert': 'DeepPavlov/rubert-base-cased',\n",
       "    'attention_probs_keep_prob': 0.5,\n",
       "    'encoder_layer_ids': [-1],\n",
       "    'optimizer': 'AdamW',\n",
       "    'optimizer_parameters': {'lr': 2e-05,\n",
       "     'weight_decay': 1e-06,\n",
       "     'betas': [0.9, 0.999],\n",
       "     'eps': 1e-06},\n",
       "    'clip_norm': 1.0,\n",
       "    'min_learning_rate': 1e-07,\n",
       "    'learning_rate_drop_patience': 30,\n",
       "    'learning_rate_drop_div': 1.5,\n",
       "    'load_before_drop': True,\n",
       "    'save_path': '../models',\n",
       "    'load_path': '../models',\n",
       "    'in': ['x_subword_tok_ids', 'attention_mask', 'startofword_markers'],\n",
       "    'in_y': ['y_ind'],\n",
       "    'out': ['y_pred_ind', 'probas']},\n",
       "   {'ref': 'tag_vocab', 'in': ['y_pred_ind'], 'out': ['y_pred']}],\n",
       "  'out': ['x_tokens', 'y_pred']},\n",
       " 'train': {'epochs': 30,\n",
       "  'batch_size': 14,\n",
       "  'metrics': [{'name': 'ner_f1', 'inputs': ['y', 'y_pred']},\n",
       "   {'name': 'ner_token_f1', 'inputs': ['y', 'y_pred']}],\n",
       "  'validation_patience': 100,\n",
       "  'val_every_n_batches': 20,\n",
       "  'log_every_n_batches': 20,\n",
       "  'show_examples': False,\n",
       "  'pytest_max_batches': 2,\n",
       "  'pytest_batch_size': 8,\n",
       "  'evaluation_targets': ['valid', 'test'],\n",
       "  'class_name': 'torch_trainer'},\n",
       " 'metadata': {'variables': {'ROOT_PATH': '~/.deeppavlov',\n",
       "   'DOWNLOADS_PATH': '~/.deeppavlov/downloads',\n",
       "   'MODELS_PATH': '~/.deeppavlov/models',\n",
       "   'TRANSFORMER': 'DeepPavlov/rubert-base-cased',\n",
       "   'MODEL_PATH': '../models'},\n",
       "  'requirements': ['c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/transformers.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/pytorch.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/torchcrf.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/sentencepiece.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/protobuf.txt']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e399f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8fc0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model(model_config, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e346a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['artfruit', '']], [['S-BRAND', 'S-TYPE']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model(['artfruit '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d8fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
