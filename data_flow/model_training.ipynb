{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57024b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 17:21:53.233 INFO in 'deeppavlov.core.data.utils'['utils'] at line 97: Downloading from http://files.deeppavlov.ai/v1/ner/ner_rus_bert_coll3_torch.tar.gz to C:\\Users\\lexan\\.deeppavlov\\models\\ner_rus_bert_coll3_torch.tar.gz\n",
      "100%|██████████| 1.44G/1.44G [02:20<00:00, 10.2MB/s]\n",
      "2025-09-23 17:24:14.675 INFO in 'deeppavlov.core.data.utils'['utils'] at line 284: Extracting C:\\Users\\lexan\\.deeppavlov\\models\\ner_rus_bert_coll3_torch.tar.gz archive into C:\\Users\\lexan\\.deeppavlov\\models\\ner_rus_bert_coll3_torch\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model('ner_collection3_bert', download=True, install=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebaef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.deeppavlov/downloads/collection3/\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import train_model, build_model \n",
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "PROJECT_DIR = '..'\n",
    "\n",
    "model_config = parse_config('ner_collection3_bert')\n",
    "\n",
    "# dataset that the model was trained on\n",
    "print(model_config['dataset_reader']['data_path'])\n",
    "\n",
    "model_config['dataset_reader']['data_path'] = PROJECT_DIR + '/datasets/conll/'\n",
    "\n",
    "del model_config['metadata']['download']\n",
    "\n",
    "model_config['metadata']['variables']['MODEL_PATH'] = PROJECT_DIR + '/models'\n",
    "\n",
    "model_config['chainer']['pipe'][1]['save_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "model_config['chainer']['pipe'][1]['load_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "\n",
    "model_config['chainer']['pipe'][2]['save_path'] = PROJECT_DIR + '/models'\n",
    "model_config['chainer']['pipe'][2]['load_path'] = PROJECT_DIR + '/models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd27e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 12:03:16.124 WARNING in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 66: TorchTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-09-24 12:03:18.857 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 104: [saving vocabulary to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\tag.dict]\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-09-24 12:03:20.316 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 151: Init from scratch. Load path C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar does not exist.\n",
      "409it [00:03, 125.66it/s]\n",
      "2025-09-24 12:03:23.605 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 198: Initial best ner_f1 of 3.8914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 3.8914, \"ner_token_f1\": 4.8383}, \"time_spent\": \"0:00:04\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 90.89it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 59.2593, \"ner_token_f1\": 71.4286}, \"time_spent\": \"0:00:05\", \"epochs_done\": 0, \"batches_seen\": 20, \"train_examples_seen\": 200, \"loss\": 1.5348522022366524}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 175.54it/s]\n",
      "2025-09-24 12:03:27.150 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 3.8914 to 54.1126\n",
      "2025-09-24 12:03:27.151 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:03:27.151 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 54.1126, \"ner_token_f1\": 73.4321}, \"time_spent\": \"0:00:07\", \"epochs_done\": 0, \"batches_seen\": 20, \"train_examples_seen\": 200, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 60.54it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 61.5385, \"ner_token_f1\": 85.7143}, \"time_spent\": \"0:00:11\", \"epochs_done\": 0, \"batches_seen\": 40, \"train_examples_seen\": 400, \"loss\": 0.6256459295749665}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 158.68it/s]\n",
      "2025-09-24 12:03:33.306 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 54.1126 to 61.9043\n",
      "2025-09-24 12:03:33.306 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:03:33.307 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 61.9043, \"ner_token_f1\": 80.2653}, \"time_spent\": \"0:00:13\", \"epochs_done\": 0, \"batches_seen\": 40, \"train_examples_seen\": 400, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 63.21it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:00:18\", \"epochs_done\": 0, \"batches_seen\": 60, \"train_examples_seen\": 600, \"loss\": 0.48272309005260466}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:03, 133.95it/s]\n",
      "2025-09-24 12:03:40.432 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 61.9043 to 70.998\n",
      "2025-09-24 12:03:40.432 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:03:40.433 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 70.998, \"ner_token_f1\": 84.8236}, \"time_spent\": \"0:00:21\", \"epochs_done\": 0, \"batches_seen\": 60, \"train_examples_seen\": 600, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 83.31it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 92.3077, \"ner_token_f1\": 92.3077}, \"time_spent\": \"0:00:25\", \"epochs_done\": 0, \"batches_seen\": 80, \"train_examples_seen\": 800, \"loss\": 0.4149527996778488}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:03, 130.98it/s]\n",
      "2025-09-24 12:03:47.645 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 70.998 to 78.6589\n",
      "2025-09-24 12:03:47.646 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:03:47.646 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 78.6589, \"ner_token_f1\": 86.0513}, \"time_spent\": \"0:00:28\", \"epochs_done\": 0, \"batches_seen\": 80, \"train_examples_seen\": 800, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 40.80it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 78.2609, \"ner_token_f1\": 93.3333}, \"time_spent\": \"0:00:33\", \"epochs_done\": 0, \"batches_seen\": 100, \"train_examples_seen\": 1000, \"loss\": 0.3445164762437344}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:03, 108.11it/s]\n",
      "2025-09-24 12:03:56.199 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 78.6589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 77.1961, \"ner_token_f1\": 86.6424}, \"time_spent\": \"0:00:36\", \"epochs_done\": 0, \"batches_seen\": 100, \"train_examples_seen\": 1000, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 71.43it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:00:38\", \"epochs_done\": 0, \"batches_seen\": 120, \"train_examples_seen\": 1200, \"loss\": 0.2986881572753191}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 137.63it/s]\n",
      "2025-09-24 12:04:00.553 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 78.6589 to 81.3412\n",
      "2025-09-24 12:04:00.554 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:04:00.555 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 81.3412, \"ner_token_f1\": 87.3126}, \"time_spent\": \"0:00:41\", \"epochs_done\": 0, \"batches_seen\": 120, \"train_examples_seen\": 1200, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 83.34it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 76.1905, \"ner_token_f1\": 93.3333}, \"time_spent\": \"0:00:45\", \"epochs_done\": 0, \"batches_seen\": 140, \"train_examples_seen\": 1400, \"loss\": 0.2794356793165207}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 141.85it/s]\n",
      "2025-09-24 12:04:07.439 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 81.3412 to 83.5258\n",
      "2025-09-24 12:04:07.439 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:04:07.440 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 83.5258, \"ner_token_f1\": 88.4916}, \"time_spent\": \"0:00:48\", \"epochs_done\": 0, \"batches_seen\": 140, \"train_examples_seen\": 1400, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 74.01it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:00:52\", \"epochs_done\": 0, \"batches_seen\": 160, \"train_examples_seen\": 1600, \"loss\": 0.2456840282306075}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:03, 134.83it/s]\n",
      "2025-09-24 12:04:14.536 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 83.5258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 78.9383, \"ner_token_f1\": 85.9665}, \"time_spent\": \"0:00:55\", \"epochs_done\": 0, \"batches_seen\": 160, \"train_examples_seen\": 1600, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 66.59it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 81.8182, \"ner_token_f1\": 83.3333}, \"time_spent\": \"0:00:56\", \"epochs_done\": 0, \"batches_seen\": 180, \"train_examples_seen\": 1800, \"loss\": 0.26771853640675547}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:03, 135.48it/s]\n",
      "2025-09-24 12:04:18.960 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 83.5258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 82.9987, \"ner_token_f1\": 88.8454}, \"time_spent\": \"0:00:59\", \"epochs_done\": 0, \"batches_seen\": 180, \"train_examples_seen\": 1800, \"impatience\": 2, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 54.17it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:01:00\", \"epochs_done\": 0, \"batches_seen\": 200, \"train_examples_seen\": 2000, \"loss\": 0.2530020704492927}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 173.51it/s]\n",
      "2025-09-24 12:04:22.610 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 83.5258 to 84.7875\n",
      "2025-09-24 12:04:22.611 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:04:22.611 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 84.7875, \"ner_token_f1\": 88.6808}, \"time_spent\": \"0:01:03\", \"epochs_done\": 0, \"batches_seen\": 200, \"train_examples_seen\": 2000, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 71.43it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 96.0, \"ner_token_f1\": 96.2963}, \"time_spent\": \"0:01:07\", \"epochs_done\": 0, \"batches_seen\": 220, \"train_examples_seen\": 2200, \"loss\": 0.261225878354162}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 168.64it/s]\n",
      "2025-09-24 12:04:28.927 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 84.7875 to 86.0903\n",
      "2025-09-24 12:04:28.927 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:04:28.927 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 86.0903, \"ner_token_f1\": 89.622}, \"time_spent\": \"0:01:09\", \"epochs_done\": 0, \"batches_seen\": 220, \"train_examples_seen\": 2200, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 75.31it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 10, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:01:13\", \"epochs_done\": 0, \"batches_seen\": 240, \"train_examples_seen\": 2400, \"loss\": 0.2566743388772011}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 174.21it/s]\n",
      "2025-09-24 12:04:35.70 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 86.0903 to 86.2749\n",
      "2025-09-24 12:04:35.70 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2025-09-24 12:04:35.71 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models.pth.tar.\n",
      "239it [01:13,  3.23it/s]\n",
      "2025-09-24 12:04:37.587 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 336: Stopped training\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "409it [00:02, 171.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 86.2749, \"ner_token_f1\": 89.7163}, \"time_spent\": \"0:00:03\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "409it [00:02, 168.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"test\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 86.5758, \"ner_token_f1\": 89.8457}, \"time_spent\": \"0:00:03\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "21395\n",
    "ner_model = train_model(model_config, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25a8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_reader': {'class_name': 'conll2003_reader',\n",
       "  'data_path': '../datasets/conll/',\n",
       "  'dataset_name': 'collection3',\n",
       "  'provide_pos': False,\n",
       "  'provide_chunk': False,\n",
       "  'iobes': True},\n",
       " 'dataset_iterator': {'class_name': 'data_learning_iterator'},\n",
       " 'chainer': {'in': ['x'],\n",
       "  'in_y': ['y'],\n",
       "  'pipe': [{'class_name': 'torch_transformers_ner_preprocessor',\n",
       "    'vocab_file': 'DeepPavlov/rubert-base-cased',\n",
       "    'do_lower_case': False,\n",
       "    'max_seq_length': 512,\n",
       "    'max_subword_length': 15,\n",
       "    'token_masking_prob': 0.0,\n",
       "    'in': ['x'],\n",
       "    'out': ['x_tokens',\n",
       "     'x_subword_tokens',\n",
       "     'x_subword_tok_ids',\n",
       "     'startofword_markers',\n",
       "     'attention_mask',\n",
       "     'tokens_offsets']},\n",
       "   {'id': 'tag_vocab',\n",
       "    'class_name': 'simple_vocab',\n",
       "    'unk_token': ['O'],\n",
       "    'pad_with_zeros': True,\n",
       "    'save_path': '../models/tag.dict',\n",
       "    'load_path': '../models/tag.dict',\n",
       "    'fit_on': ['y'],\n",
       "    'in': ['y'],\n",
       "    'out': ['y_ind']},\n",
       "   {'class_name': 'torch_transformers_sequence_tagger',\n",
       "    'n_tags': '#tag_vocab.len',\n",
       "    'pretrained_bert': 'DeepPavlov/rubert-base-cased',\n",
       "    'attention_probs_keep_prob': 0.5,\n",
       "    'encoder_layer_ids': [-1],\n",
       "    'optimizer': 'AdamW',\n",
       "    'optimizer_parameters': {'lr': 2e-05,\n",
       "     'weight_decay': 1e-06,\n",
       "     'betas': [0.9, 0.999],\n",
       "     'eps': 1e-06},\n",
       "    'clip_norm': 1.0,\n",
       "    'min_learning_rate': 1e-07,\n",
       "    'learning_rate_drop_patience': 30,\n",
       "    'learning_rate_drop_div': 1.5,\n",
       "    'load_before_drop': True,\n",
       "    'save_path': '../models',\n",
       "    'load_path': '../models',\n",
       "    'in': ['x_subword_tok_ids', 'attention_mask', 'startofword_markers'],\n",
       "    'in_y': ['y_ind'],\n",
       "    'out': ['y_pred_ind', 'probas']},\n",
       "   {'ref': 'tag_vocab', 'in': ['y_pred_ind'], 'out': ['y_pred']}],\n",
       "  'out': ['x_tokens', 'y_pred']},\n",
       " 'train': {'epochs': 30,\n",
       "  'batch_size': 10,\n",
       "  'metrics': [{'name': 'ner_f1', 'inputs': ['y', 'y_pred']},\n",
       "   {'name': 'ner_token_f1', 'inputs': ['y', 'y_pred']}],\n",
       "  'validation_patience': 100,\n",
       "  'val_every_n_batches': 20,\n",
       "  'log_every_n_batches': 20,\n",
       "  'show_examples': False,\n",
       "  'pytest_max_batches': 2,\n",
       "  'pytest_batch_size': 8,\n",
       "  'evaluation_targets': ['valid', 'test'],\n",
       "  'class_name': 'torch_trainer'},\n",
       " 'metadata': {'variables': {'ROOT_PATH': '~/.deeppavlov',\n",
       "   'DOWNLOADS_PATH': '~/.deeppavlov/downloads',\n",
       "   'MODELS_PATH': '~/.deeppavlov/models',\n",
       "   'TRANSFORMER': 'DeepPavlov/rubert-base-cased',\n",
       "   'MODEL_PATH': '../models'},\n",
       "  'requirements': ['c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/protobuf.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/torchcrf.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/transformers.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/sentencepiece.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/pytorch.txt']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e399f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8fc0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model(model_config, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e346a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['artfruit', 'нектари']], [['S-BRAND', 'S-TYPE']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model(['artfruit нектари'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d8fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
