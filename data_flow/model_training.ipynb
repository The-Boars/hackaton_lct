{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57024b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 17:21:53.233 INFO in 'deeppavlov.core.data.utils'['utils'] at line 97: Downloading from http://files.deeppavlov.ai/v1/ner/ner_rus_bert_coll3_torch.tar.gz to C:\\Users\\lexan\\.deeppavlov\\models\\ner_rus_bert_coll3_torch.tar.gz\n",
      "100%|██████████| 1.44G/1.44G [02:20<00:00, 10.2MB/s]\n",
      "2025-09-23 17:24:14.675 INFO in 'deeppavlov.core.data.utils'['utils'] at line 284: Extracting C:\\Users\\lexan\\.deeppavlov\\models\\ner_rus_bert_coll3_torch.tar.gz archive into C:\\Users\\lexan\\.deeppavlov\\models\\ner_rus_bert_coll3_torch\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model('ner_collection3_bert', download=True, install=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebaef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.deeppavlov/downloads/collection3/\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import train_model, build_model \n",
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "PROJECT_DIR = '..'\n",
    "MODEL_NAME = 'model-78'\n",
    "\n",
    "model_config = parse_config('ner_collection3_bert')\n",
    "\n",
    "# dataset that the model was trained on\n",
    "print(model_config['dataset_reader']['data_path'])\n",
    "\n",
    "model_config['dataset_reader']['data_path'] = PROJECT_DIR + '/datasets/conll/'\n",
    "\n",
    "del model_config['metadata']['download']\n",
    "\n",
    "model_config['metadata']['variables']['MODEL_PATH'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "\n",
    "model_config['chainer']['pipe'][1]['save_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "model_config['chainer']['pipe'][1]['load_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "\n",
    "model_config['chainer']['pipe'][2]['save_path'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "model_config['chainer']['pipe'][2]['load_path'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "\n",
    "model_config['train']['batch_size'] = 500\n",
    "\n",
    "model_config['train']['log_every_n_batches'] = 10\n",
    "model_config['train']['val_every_n_batches'] = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd27e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 10:01:57.297 WARNING in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 66: TorchTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
      "2025-09-25 10:01:57.805 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 104: [saving vocabulary to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\models\\tag.dict]\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "9it [00:00,  9.97it/s]\n",
      "2025-09-25 10:02:01.831 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 198: Initial best ner_f1 of 94.3631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 94.3631, \"ner_token_f1\": 95.5484}, \"time_spent\": \"0:00:01\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 500, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:00:04\", \"epochs_done\": 0, \"batches_seen\": 10, \"train_examples_seen\": 5000, \"loss\": 0.004319065366871655}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [00:00,  9.95it/s]\n",
      "2025-09-25 10:02:05.91 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 94.3631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 93.8699, \"ner_token_f1\": 95.2116}, \"time_spent\": \"0:00:05\", \"epochs_done\": 0, \"batches_seen\": 10, \"train_examples_seen\": 5000, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 10.35it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 500, \"metrics\": {\"ner_f1\": 99.6678, \"ner_token_f1\": 99.7786}, \"time_spent\": \"0:00:07\", \"epochs_done\": 0, \"batches_seen\": 20, \"train_examples_seen\": 10000, \"loss\": 0.0038081371458247304}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [00:00,  6.83it/s]\n",
      "19it [00:06,  2.88it/s]\n",
      "2025-09-25 10:02:08.423 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 336: Stopped training\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "9it [00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 94.3631, \"ner_token_f1\": 95.5484}, \"time_spent\": \"0:00:01\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"test\": {\"eval_examples_count\": 4088, \"metrics\": {\"ner_f1\": 93.5014, \"ner_token_f1\": 94.8994}, \"time_spent\": \"0:00:01\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ner_model = train_model(model_config, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25a8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_reader': {'class_name': 'conll2003_reader',\n",
       "  'data_path': '../datasets/conll/',\n",
       "  'dataset_name': 'collection3',\n",
       "  'provide_pos': False,\n",
       "  'provide_chunk': False,\n",
       "  'iobes': True},\n",
       " 'dataset_iterator': {'class_name': 'data_learning_iterator'},\n",
       " 'chainer': {'in': ['x'],\n",
       "  'in_y': ['y'],\n",
       "  'pipe': [{'class_name': 'torch_transformers_ner_preprocessor',\n",
       "    'vocab_file': 'DeepPavlov/rubert-base-cased',\n",
       "    'do_lower_case': False,\n",
       "    'max_seq_length': 512,\n",
       "    'max_subword_length': 15,\n",
       "    'token_masking_prob': 0.0,\n",
       "    'in': ['x'],\n",
       "    'out': ['x_tokens',\n",
       "     'x_subword_tokens',\n",
       "     'x_subword_tok_ids',\n",
       "     'startofword_markers',\n",
       "     'attention_mask',\n",
       "     'tokens_offsets']},\n",
       "   {'id': 'tag_vocab',\n",
       "    'class_name': 'simple_vocab',\n",
       "    'unk_token': ['O'],\n",
       "    'pad_with_zeros': True,\n",
       "    'save_path': '../models/tag.dict',\n",
       "    'load_path': '../models/tag.dict',\n",
       "    'fit_on': ['y'],\n",
       "    'in': ['y'],\n",
       "    'out': ['y_ind']},\n",
       "   {'class_name': 'torch_transformers_sequence_tagger',\n",
       "    'n_tags': '#tag_vocab.len',\n",
       "    'pretrained_bert': 'DeepPavlov/rubert-base-cased',\n",
       "    'attention_probs_keep_prob': 0.5,\n",
       "    'encoder_layer_ids': [-1],\n",
       "    'optimizer': 'AdamW',\n",
       "    'optimizer_parameters': {'lr': 2e-05,\n",
       "     'weight_decay': 1e-06,\n",
       "     'betas': [0.9, 0.999],\n",
       "     'eps': 1e-06},\n",
       "    'clip_norm': 1.0,\n",
       "    'min_learning_rate': 1e-07,\n",
       "    'learning_rate_drop_patience': 30,\n",
       "    'learning_rate_drop_div': 1.5,\n",
       "    'load_before_drop': True,\n",
       "    'save_path': '../models',\n",
       "    'load_path': '../models',\n",
       "    'in': ['x_subword_tok_ids', 'attention_mask', 'startofword_markers'],\n",
       "    'in_y': ['y_ind'],\n",
       "    'out': ['y_pred_ind', 'probas']},\n",
       "   {'ref': 'tag_vocab', 'in': ['y_pred_ind'], 'out': ['y_pred']}],\n",
       "  'out': ['x_tokens', 'y_pred']},\n",
       " 'train': {'epochs': 30,\n",
       "  'batch_size': 14,\n",
       "  'metrics': [{'name': 'ner_f1', 'inputs': ['y', 'y_pred']},\n",
       "   {'name': 'ner_token_f1', 'inputs': ['y', 'y_pred']}],\n",
       "  'validation_patience': 100,\n",
       "  'val_every_n_batches': 20,\n",
       "  'log_every_n_batches': 20,\n",
       "  'show_examples': False,\n",
       "  'pytest_max_batches': 2,\n",
       "  'pytest_batch_size': 8,\n",
       "  'evaluation_targets': ['valid', 'test'],\n",
       "  'class_name': 'torch_trainer'},\n",
       " 'metadata': {'variables': {'ROOT_PATH': '~/.deeppavlov',\n",
       "   'DOWNLOADS_PATH': '~/.deeppavlov/downloads',\n",
       "   'MODELS_PATH': '~/.deeppavlov/models',\n",
       "   'TRANSFORMER': 'DeepPavlov/rubert-base-cased',\n",
       "   'MODEL_PATH': '../models'},\n",
       "  'requirements': ['c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/transformers.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/pytorch.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/torchcrf.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/sentencepiece.txt',\n",
       "   'c:\\\\Users\\\\lexan\\\\miniconda3\\\\envs\\\\dp310\\\\lib\\\\site-packages\\\\deeppavlov/requirements/protobuf.txt']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e399f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8fc0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model(model_config, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e346a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['artfruit', 'нектари']], [['S-BRAND', 'S-TYPE']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model(['artfruit нектари'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d8fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
