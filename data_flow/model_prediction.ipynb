{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d10f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.deeppavlov/downloads/collection3/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import train_model, build_model \n",
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "PROJECT_DIR = '..'\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "\n",
    "# dataset that the model was trained on\n",
    "model_config = parse_config('ner_collection3_bert')\n",
    "\n",
    "# dataset that the model was trained on\n",
    "print(model_config['dataset_reader']['data_path'])\n",
    "\n",
    "model_config['dataset_reader']['data_path'] = PROJECT_DIR + '/datasets/conll/'\n",
    "\n",
    "del model_config['metadata']['download']\n",
    "\n",
    "\n",
    "model_config['dataset_reader']['iobes'] = False\n",
    "model_config['metadata']['variables']['MODEL_PATH'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "\n",
    "model_config['chainer']['pipe'][1]['save_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "model_config['chainer']['pipe'][1]['load_path'] = PROJECT_DIR + '/models/tag.dict'\n",
    "\n",
    "model_config['chainer']['pipe'][2]['save_path'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "model_config['chainer']['pipe'][2]['load_path'] = PROJECT_DIR + '/models/' + MODEL_NAME\n",
    "\n",
    "\n",
    "model_config['train']['batch_size'] = 400\n",
    "\n",
    "model_config['train']['log_every_n_batches'] = 10\n",
    "model_config['train']['val_every_n_batches'] = 10\n",
    "\n",
    "\n",
    "model_config['chainer']['pipe'][0]['in'] = ['x_tokens']\n",
    "model_config['chainer']['pipe'].insert(0, {\"id\": \"ws_tok\", \"class_name\": \"split_tokenizer\", \"in\": [\"x\"], \"out\": [\"x_tokens\"]})\n",
    "\n",
    "ner_model = build_model(model_config, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b967db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexan\\miniconda3\\envs\\dp310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ner_model2 = build_model(model_config, download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc904e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc7cee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs: 1000, warmup: 5\n",
      "mean: 0.004872s\n",
      "median: 0.004767s\n",
      "stdev: 0.000781s\n",
      "min: 0.004451s, max: 0.025998s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics as stats\n",
    "\n",
    "def benchmark_model(model, sample, warmup=3, runs=20):\n",
    "    # прогрев — избавляемся от накладных расходов первого вызова\n",
    "    for _ in range(warmup):\n",
    "        model(sample)\n",
    "\n",
    "    timings = []\n",
    "    for _ in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        model(sample)\n",
    "        timings.append(time.perf_counter() - t0)\n",
    "\n",
    "    print(f\"runs: {runs}, warmup: {warmup}\")\n",
    "    print(f\"mean: {stats.mean(timings):.6f}s\")\n",
    "    print(f\"median: {stats.median(timings):.6f}s\")\n",
    "    print(f\"stdev: {stats.pstdev(timings):.6f}s\")\n",
    "    print(f\"min: {min(timings):.6f}s, max: {max(timings):.6f}s\")\n",
    "\n",
    "# пример вызова\n",
    "benchmark_model(ner_model, ['форма для выпечки'], 5, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c489bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs: 10000, warmup: 0\n",
      "mean: 0.004721s\n",
      "median: 0.004675s\n",
      "stdev: 0.000292s\n",
      "min: 0.004414s, max: 0.012190s\n"
     ]
    }
   ],
   "source": [
    "benchmark_model(ner_model, ['форма для выпечки'], 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ccc8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches: 20, concurrency: 100\n",
      "mean: 0.648246s\n",
      "median: 0.637933s\n",
      "stdev: 0.059266s\n",
      "min: 0.005023s, max: 0.805383s\n",
      "p95: 0.750717s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics as stats\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def _call_once(model, sample):\n",
    "    start = time.perf_counter()\n",
    "    model(sample)\n",
    "    end = time.perf_counter()\n",
    "    return start, end\n",
    "\n",
    "def benchmark_concurrent(model, sample, qps=100, warmup_batches=2,\n",
    "                         measurement_batches=20, batch_gap=0.0):\n",
    "    \"\"\"Имитация нагрузки ~qps запросов/сек; sample — список токенов для модели.\"\"\"\n",
    "    # прогрев (батчи не учитываются в статистике)\n",
    "    with ThreadPoolExecutor(max_workers=qps) as executor:\n",
    "        for _ in range(warmup_batches):\n",
    "            futures = [executor.submit(_call_once, model, sample) for _ in range(qps)]\n",
    "            for future in as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "    timings = []\n",
    "    with ThreadPoolExecutor(max_workers=qps) as executor:\n",
    "        for _ in range(measurement_batches):\n",
    "            batch_start = time.perf_counter()\n",
    "            futures = [executor.submit(_call_once, model, sample) for _ in range(qps)]\n",
    "            batch_durations = []\n",
    "            for future in as_completed(futures):\n",
    "                start, end = future.result()\n",
    "                batch_durations.append(end - start)\n",
    "                timings.append(end - start)\n",
    "\n",
    "            batch_elapsed = time.perf_counter() - batch_start\n",
    "            if batch_elapsed < batch_gap:\n",
    "                time.sleep(batch_gap - batch_elapsed)\n",
    "\n",
    "    print(f\"batches: {measurement_batches}, concurrency: {qps}\")\n",
    "    print(f\"mean: {stats.mean(timings):.6f}s\")\n",
    "    print(f\"median: {stats.median(timings):.6f}s\")\n",
    "    print(f\"stdev: {stats.pstdev(timings):.6f}s\")\n",
    "    print(f\"min: {min(timings):.6f}s, max: {max(timings):.6f}s\")\n",
    "    timings_sorted = sorted(timings)\n",
    "    p95 = timings_sorted[int(len(timings_sorted) * 0.95)]\n",
    "    print(f\"p95: {p95:.6f}s\")\n",
    "\n",
    "# пример вызова\n",
    "benchmark_concurrent(ner_model, ['форма для выпечки'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e781de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to C:\\Users\\lexan\\OneDrive\\Documents\\hackaton_lct\\datasets\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "SUBMISSION_PATH = Path(PROJECT_DIR) / 'datasets' / 'submission_raw.csv'\n",
    "OUTPUT_PATH = Path(PROJECT_DIR) / 'datasets' / 'submission.csv'\n",
    "\n",
    "TOKEN_PATTERN = re.compile(r'\\S+')\n",
    "\n",
    "def normalize_tag(tag: str) -> str:\n",
    "    if not isinstance(tag, str):\n",
    "        return 'O'\n",
    "    tag = tag.strip()\n",
    "    if not tag:\n",
    "        return 'O'\n",
    "    upper_tag = tag.upper()\n",
    "    # keep O as-is; no S/E mapping anymore\n",
    "    if upper_tag == 'O':\n",
    "        return 'O'\n",
    "    # pass B-/I- through, preserving the type suffix\n",
    "    if upper_tag.startswith('B-') or upper_tag.startswith('I-'):\n",
    "        return tag\n",
    "    # anything else is unexpected; return as-is\n",
    "    return tag\n",
    "\n",
    "def is_tag(value) -> bool:\n",
    "    if not isinstance(value, str):\n",
    "        return False\n",
    "    candidate = value.strip().upper()\n",
    "    if not candidate:\n",
    "        return False\n",
    "    if candidate == 'O':\n",
    "        return True\n",
    "    return (candidate == 'O') or (len(candidate) >= 3 and candidate[1] == '-' and candidate[0] in {'B','I'})\n",
    "\n",
    "def looks_like_sequence(seq, predicate) -> bool:\n",
    "    if not isinstance(seq, (list, tuple)) or not seq:\n",
    "        return False\n",
    "    return all(predicate(item) for item in seq)\n",
    "\n",
    "def looks_like_tag_sequence(seq) -> bool:\n",
    "    return looks_like_sequence(seq, is_tag)\n",
    "\n",
    "def looks_like_token_sequence(seq) -> bool:\n",
    "    return looks_like_sequence(seq, lambda item: isinstance(item, str) and not is_tag(item))\n",
    "\n",
    "def extract_tokens_and_tags(prediction) -> tuple[list[str], list[str]]:\n",
    "    if isinstance(prediction, tuple):\n",
    "        prediction = list(prediction)\n",
    "    if not isinstance(prediction, list):\n",
    "        raise ValueError(f'Unexpected model output type: {type(prediction)}')\n",
    "    tokens: list[str] = []\n",
    "    tags: list[str] = []\n",
    "\n",
    "    def traverse(node):\n",
    "        nonlocal tokens, tags\n",
    "        if isinstance(node, tuple):\n",
    "            node = list(node)\n",
    "        if isinstance(node, list):\n",
    "            if not tokens and looks_like_token_sequence(node):\n",
    "                tokens = [str(item) for item in node]\n",
    "            if not tags and looks_like_tag_sequence(node):\n",
    "                tags = [normalize_tag(str(item)) for item in node]\n",
    "            if tokens and tags:\n",
    "                return\n",
    "            for child in node:\n",
    "                traverse(child)\n",
    "\n",
    "    traverse(prediction)\n",
    "    if not tags:\n",
    "        raise ValueError(f'Unable to extract tag sequence from model output: {prediction}')\n",
    "    return tokens, tags\n",
    "\n",
    "def compute_annotation(text: str, tokens: list[str], tags: list[str]) -> list[tuple[int, int, str]]:\n",
    "    if not tags:\n",
    "        return []\n",
    "    if tokens:\n",
    "        effective_len = min(len(tokens), len(tags))\n",
    "        tokens = tokens[:effective_len]\n",
    "        tags = tags[:effective_len]\n",
    "    annotation: list[tuple[int, int, str]] = []\n",
    "    if tokens:\n",
    "        cursor = 0\n",
    "        fallback = False\n",
    "        for token, tag in zip(tokens, tags):\n",
    "            token = token or ''\n",
    "            if not tag:\n",
    "                cursor += len(token)\n",
    "                continue\n",
    "            start = text.find(token, cursor)\n",
    "            if start == -1:\n",
    "                fallback = True\n",
    "                break\n",
    "            end = start + len(token)\n",
    "            annotation.append((start, end, tag))\n",
    "            cursor = end\n",
    "        if fallback:\n",
    "            annotation = []\n",
    "    if not annotation:\n",
    "        matches = list(TOKEN_PATTERN.finditer(text))\n",
    "        effective_len = min(len(matches), len(tags))\n",
    "        for match, tag in zip(matches[:effective_len], tags[:effective_len]):\n",
    "            if not tag:\n",
    "                continue\n",
    "            annotation.append((match.start(), match.end(), tag))\n",
    "    return annotation\n",
    "\n",
    "submission_df = pd.read_csv(SUBMISSION_PATH, sep=';', encoding='utf-8')\n",
    "annotations = []\n",
    "for row_idx, sample in enumerate(submission_df['sample'], start=1):\n",
    "    text = '' if pd.isna(sample) else str(sample)\n",
    "    model_output = ner_model([text])\n",
    "    tokens, tags = extract_tokens_and_tags(model_output)\n",
    "    annotation = compute_annotation(text, tokens, tags)\n",
    "    word_count = len(tokens) if tokens else len(TOKEN_PATTERN.findall(text))\n",
    "    entity_count = len(annotation)\n",
    "    if word_count != entity_count:\n",
    "        print(f'Row {row_idx}: word count {word_count} != annotation entities {entity_count}')\n",
    "    annotations.append(annotation)\n",
    "\n",
    "submission_df['annotation'] = [str(ann) for ann in annotations]\n",
    "submission_df[['sample', 'annotation']].to_csv(OUTPUT_PATH, sep=';', encoding='utf-8', index=False)\n",
    "print(f'Saved predictions to {OUTPUT_PATH.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d492b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891117ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv('../datasets/submission_raw.csv', sep=';')\n",
    "submission.loc[:, 'annotation'] = \"\"\n",
    "submission.to_csv('../datasets/submission_empty.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d90309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nРќР°РїРёС€Рё РєРѕРґ, РєРѕС‚РѕСЂС‹Р№ РїСЂРѕР№РґРµС‚СЃСЏ РїРѕ РІСЃРµРј СЃС‚РѕР»Р±С†Р° 'sample' РІ submission, РїСЂРёРјРµРЅРёС‚ ner_model Рє РєР°Р¶РґРѕРјСѓ РёР· РЅРёС….\\nРќР° РІС‹С…РѕРґРµ РёР· ner_model РїРѕР»СѓС‡Р°РµС‚СЃСЏ СЃРїРёСЃРѕРє СЃС‚СЂРѕРє СЃ С‚РёРїР°РјРё СЃСѓС‰РЅРѕСЃС‚РµР№. РќСѓР¶РЅРѕ РїСЂРµРѕР±СЂР°Р·РѕРІР°С‚СЊ СЌС‚Рѕ РІ РЅРѕРІС‹Р№ С„РѕСЂРјР°С‚.\\nР’Рѕ-РїРµСЂРІС‹С…, РЅСѓР¶РЅРѕ Р·Р°РјРµРЅРёС‚СЊ РІСЃРµ S- РЅР° B-, Р° РІСЃРµ E- РЅР° I-.\\nР’Рѕ-РІС‚РѕСЂС‹С…, РЅСѓР¶РЅРѕ СЃРґРµР»Р°С‚СЊ Р°РЅРЅРѕС‚Р°С†РёСЋ С„РѕСЂРјР°С‚Р° [(РёРЅРґРµРєСЃ РЅР°С‡Р°Р»Р° СЃСѓС‰РЅРѕСЃС‚Рё, РёРЅРґРµРєСЃ РєРѕРЅС†Р° СЃСѓС‰РЅРѕСЃС‚Рё, СЃС‚СЂРѕРєР° С‚РёРїР°),] Р№РѕРіСѓСЂС‚С‹ РїРёС‚СЊРµРІС‹\\t[(0, 7, 'B-TYPE'), (8, 15, 'I-TYPE')]\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "РќР°РїРёС€Рё РєРѕРґ, РєРѕС‚РѕСЂС‹Р№ РїСЂРѕР№РґРµС‚СЃСЏ РїРѕ РІСЃРµРј СЃС‚РѕР»Р±С†Р° 'sample' РІ submission, РїСЂРёРјРµРЅРёС‚ ner_model Рє РєР°Р¶РґРѕРјСѓ РёР· РЅРёС….\n",
    "РќР° РІС‹С…РѕРґРµ РёР· ner_model РїРѕР»СѓС‡Р°РµС‚СЃСЏ СЃРїРёСЃРѕРє СЃС‚СЂРѕРє СЃ С‚РёРїР°РјРё СЃСѓС‰РЅРѕСЃС‚РµР№. РќСѓР¶РЅРѕ РїСЂРµРѕР±СЂР°Р·РѕРІР°С‚СЊ СЌС‚Рѕ РІ РЅРѕРІС‹Р№ С„РѕСЂРјР°С‚.\n",
    "Р’Рѕ-РїРµСЂРІС‹С…, РЅСѓР¶РЅРѕ Р·Р°РјРµРЅРёС‚СЊ РІСЃРµ S- РЅР° B-, Р° РІСЃРµ E- РЅР° I-.\n",
    "Р’Рѕ-РІС‚РѕСЂС‹С…, РЅСѓР¶РЅРѕ СЃРґРµР»Р°С‚СЊ Р°РЅРЅРѕС‚Р°С†РёСЋ С„РѕСЂРјР°С‚Р° [(РёРЅРґРµРєСЃ РЅР°С‡Р°Р»Р° СЃСѓС‰РЅРѕСЃС‚Рё, РёРЅРґРµРєСЃ РєРѕРЅС†Р° СЃСѓС‰РЅРѕСЃС‚Рё, СЃС‚СЂРѕРєР° С‚РёРїР°),] \n",
    "РќР°РїСЂРёРјРµСЂ, РґР»СЏ СЃС‚СЂРѕРєРё \"Р№РѕРіСѓСЂС‚С‹ РїРёС‚СЊРµРІС‹\" Р°РЅРЅРѕС‚Р°С†РёСЏ Р±СѓРґРµС‚ [(0, 7, 'B-TYPE'), (8, 15, 'I-TYPE')], РёРЅРґРµРєСЃС‹ РЅР°С‡Р°Р»Р° Рё РєРѕРЅС†Р° СЃСѓС‰РЅРѕСЃС‚Рё СЂР°Р±РѕС‚Р°СЋС‚ РєР°Рє СЃСЂРµР·С‹ РІ РїРёС‚РѕРЅРµ - РІРєР»СЋС‡РёС‚РµР»СЊРЅРѕ-РёСЃРєР»СЋС‡РёС‚РµР»СЊРЅРѕ.\n",
    "РўРѕ РµСЃС‚СЊ, 0 РёРЅРґРµРєСЃ СЌС‚Рѕ Р±СѓРєРІР° \"Р№\", 7 РёРЅРґРµРєСЃ - СЌС‚Рѕ РїСЂРѕР±РµР» РїРѕСЃР»Рµ СЃР»РѕРІР° \"Р№РѕРіСѓСЂС‚С‹\", РЅРѕ РїСЂРѕР±РµР» РІ СЃР°РјСѓ СЃСѓС‰РЅРѕСЃС‚СЊ РЅРµ РІС…РѕРґРёС‚.\n",
    "Р’ РЅРѕРІС‹Р№ С„Р°Р№Р» submission_final.csv РЅСѓР¶РЅРѕ РІ С‚РѕРј Р¶Рµ РїРѕСЂСЏРґРєРµ Р·Р°РїРёСЃС‹РІР°С‚СЊ СЃС‚РѕР»Р±С†С‹ sample Рё annotation (Р°РЅРЅРѕС‚Р°С†РёРё СѓР¶Рµ РІ РЅРѕРІРѕРј С„РѕСЂРјР°С‚Рµ)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
